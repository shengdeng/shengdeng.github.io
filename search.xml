<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Linux基本操作：vim编辑器]]></title>
      <url>http://shengdeng.github.io/2016/06/16/Linux%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%EF%BC%9Avim%E7%BC%96%E8%BE%91%E5%99%A8/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<p>vi是一个命令行界面下的文本编辑器，Vim是从vi发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。和Emacs并列成为类Unix系统用户最喜欢的编辑器。</p>
<pre><code>vim 目标文件路径
</code></pre><ul>
<li>如果目标文件存在，则打开目标文件</li>
<li>如果不存在，则创建文件</li>
</ul>
<h1 id="1-vim的三种模式"><a href="#1-vim的三种模式" class="headerlink" title="1. vim的三种模式"></a>1. vim的三种模式</h1><h2 id="命令模式"><a href="#命令模式" class="headerlink" title="命令模式"></a>命令模式</h2><ul>
<li>vim 启动默认进入命令模式</li>
<li>命令模式功能：选择、复制、粘贴、撤销</li>
<li>其他模式通过<code>esc</code>可以进入<strong>命令模式</strong></li>
<li>命令模式按<code>i</code>进入<strong>插入模式</strong></li>
<li>命令模式按<code>:</code>进入<strong>ex模式</strong></li>
</ul>
<p><strong>ex模式</strong>→<code>esc</code>→<strong>命令模式</strong>←<code>esc</code>←<strong>插入模式</strong></p>
<p><strong>ex模式</strong>←<code>：</code>←<strong>命令模式</strong>→<code>i</code>→<strong>插入模式</strong></p>
<table>
<thead>
<tr>
<th>命令</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>i</td>
<td>进入插入模式，在光标处插入</td>
</tr>
<tr>
<td>o</td>
<td>进入插入模式，在当前行的下面插入新行</td>
</tr>
<tr>
<td>dd</td>
<td><strong>删除</strong>整行</td>
</tr>
<tr>
<td>u</td>
<td><strong>撤销</strong></td>
</tr>
<tr>
<td>yy</td>
<td><strong>复制</strong>当前行</td>
</tr>
</tbody>
</table>
<h2 id="插入模式"><a href="#插入模式" class="headerlink" title="插入模式"></a>插入模式</h2><ul>
<li><code>esc</code>回到命令模式</li>
<li>编辑文本</li>
</ul>
<h2 id="ex模式"><a href="#ex模式" class="headerlink" title="ex模式"></a>ex模式</h2><ul>
<li>ex模式功能：保存、退出</li>
</ul>
<table>
<thead>
<tr>
<th>命令</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>:w</td>
<td><strong>保存</strong>修改</td>
</tr>
<tr>
<td>:q</td>
<td><strong>退出</strong></td>
</tr>
<tr>
<td>:wq或:x</td>
<td>保存并退出</td>
</tr>
<tr>
<td>:q!</td>
<td><strong>强制退出不保存</strong></td>
</tr>
<tr>
<td>:set number</td>
<td>显示行号</td>
</tr>
<tr>
<td>:sh</td>
<td>切换到命令行界面；ctrl+d切回vim</td>
</tr>
</tbody>
</table>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux权限管理：sudo命令]]></title>
      <url>http://shengdeng.github.io/2016/06/16/Linux%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%EF%BC%9Asudo%E6%9D%83%E9%99%90/</url>
      <content type="html"><![CDATA[<p>使用su切换用户时需知晓对应用户的登陆密码，即若切换成root用户身份，需知道root用户的登陆密码。作为root用户管理员，如何授权其他普通用户，<strong>在不需要知晓root密码的情况下，执行root权限的命令操作？</strong>，此时即可使用sudo。</p>
<a id="more"></a>
<h1 id="1-什么是sudo权限"><a href="#1-什么是sudo权限" class="headerlink" title="1.什么是sudo权限"></a>1.什么是sudo权限</h1><p>生产环境中，root用户是团队的负责人，因为root权限过于强大。但是每天工作的普通管理员日常操作常常权限限制，像重启、备份、tar等命令。在linux中普通用户受限严重，很多简单的命令都有权限限制。</p>
<ul>
<li>root用户把本来只能<strong>超级用户执行的命令</strong>赋予<strong>普通用户执行</strong></li>
<li>sudo的操作对象是系统命令</li>
<li><strong>文件的权限UGO、特殊权限</strong>作用是：用户能不能操作<strong>文件</strong></li>
</ul>
<blockquote>
<p>总之，<strong>sudo命令</strong>的作用是：切换到root用户去执行命令！</p>
</blockquote>
<h2 id="1-1-etc-sudoers配置文件"><a href="#1-1-etc-sudoers配置文件" class="headerlink" title="1.1 /etc/sudoers配置文件"></a>1.1 /etc/sudoers配置文件</h2><ul>
<li>sudo是一种权限管理机制，依赖于<strong>/etc/sudoers</strong>，其定义了<ul>
<li>授权给<strong>哪个用户</strong></li>
<li>可以<strong>以root身份</strong></li>
<li>能够<strong>执行什么管理命令</strong></li>
</ul>
</li>
<li><strong>root用户</strong>通过<strong>使用visudo命令</strong>编辑sudo的配置文件/etc/sudoers，才可以授权其他普通用户执行sudo命令。</li>
<li>visudo命令可以提供basic sanitychecks和check for parse errors，即提供快速的正确性有效性检查，以及语法检查功能。</li>
</ul>
<p>查看/etc/sudoers文件</p>
<pre><code>[root@localhost stefan]# visudo
</code></pre><p>内容很多其中有如下几行比较重要：</p>
<h3 id="授权用户模板格式"><a href="#授权用户模板格式" class="headerlink" title="授权用户模板格式"></a>授权用户模板格式</h3><p><code>root    ALL=(ALL)    ALL</code>一行就是授权用户的模板格式。</p>
<pre><code>## Allow root to run any commands anywhere
root    ALL=(ALL)   ALL
#要授权的用户  需要生效的主机名或IP=(可切换的身份)  可执行的命令
</code></pre><ul>
<li>ALL即<strong>所有都ok</strong>。所以以上语句含义是：<strong>root用户</strong>(who) 在 <strong>所有主机</strong>(where) 上可以 <strong>切换成所有用户</strong>，<strong>执行所有命令</strong>(do what)。</li>
<li>所有主机（<strong>第一个ALL</strong>）的含义是在集群场景中，以上授权在哪些主机上可以生效；单台主机用ALL就行。</li>
<li>系统命令只能root执行。所以<strong>第二个ALL</strong>一般写成（root），即普通用户自动切换成root用户后执行系统命令。若省略默认为（root）。</li>
<li><strong>第三个ALL</strong>，命令写的越简单（比如写ALL），普通用户得到的权限越大，越不安全。<strong>写的越详细，包括命令的参数也写上，普通用户得到的权限越小，越安全。</strong>有些centos版本规定命令最好用绝对路径。用<code>whereis 命令</code>可以查到命令的绝对路径</li>
</ul>
<blockquote>
<p><strong>Tips:</strong> 如果是自己的系统，又不想每次都切换root输入密码，一个命令一个命令的配置visudo很麻烦，可以改成</p>
<pre><code>用户名   ALL=(ALL)   ALL
</code></pre><p>这样，直接输入命令就是普通用户，加上sudo就是超级用户。免去用su命令来回切换的麻烦。但是，生产环境一定要小心配置visudo原因见第2节。</p>
</blockquote>
<h3 id="授权组模板格式"><a href="#授权组模板格式" class="headerlink" title="授权组模板格式"></a>授权组模板格式</h3><p><code>%wheel    ALL=(ALL)    ALL</code>就是授权用户的模板格式。</p>
<pre><code>## Allows people in group wheel to run all commands
#允许组wheel的组成员运行所有的命令
%wheel  ALL=(ALL)   ALL

## Same thing without a password
#允许组wheel的组成员运行所有的命令，并且不需要密码
%wheel    ALL=(ALL)   NOPASSWD: ALL
</code></pre><p>%百分号不能少</p>
<h2 id="1-2-查看sudo可执行的命令"><a href="#1-2-查看sudo可执行的命令" class="headerlink" title="1.2 查看sudo可执行的命令"></a>1.2 查看sudo可执行的命令</h2><pre><code>$sudo -l #查看本用户可以执行的sudo命令
</code></pre><h1 id="2-sudo命令的危险"><a href="#2-sudo命令的危险" class="headerlink" title="2. sudo命令的危险"></a>2. sudo命令的危险</h1><h2 id="实例1-授权普通用户：可以重启服务器（shutdown命令）"><a href="#实例1-授权普通用户：可以重启服务器（shutdown命令）" class="headerlink" title="实例1. 授权普通用户：可以重启服务器（shutdown命令）"></a>实例1. 授权普通用户：可以重启服务器（shutdown命令）</h2><p>切换到root用户，执行visudo命令，增加一行：</p>
<pre><code>user1    ALL=(ALL)    /bin/shutdown -r now
</code></pre><p>此时，user1只能重启，如果写成：</p>
<pre><code>user1    ALL=(ALL)    /bin/shutdown
</code></pre><p>那么user1不光可以重启-r，还可以关机和其他任何选项。</p>
<h2 id="实例2-授权普通用户：可以添加其他用户（useradd命令）可以修改其他用户密码（passwd-用户名）"><a href="#实例2-授权普通用户：可以添加其他用户（useradd命令）可以修改其他用户密码（passwd-用户名）" class="headerlink" title="实例2. 授权普通用户：可以添加其他用户（useradd命令）可以修改其他用户密码（passwd 用户名）"></a>实例2. 授权普通用户：可以添加其他用户（useradd命令）可以修改其他用户密码（passwd 用户名）</h2><pre><code>user1 ALL=/usr/sbin/useradd    #(ALL)缺省默认为(root)，即切换成root用户

user1 ALL=/usr/bin/passwd    #授予普通用户以root用户身份修改其他所有用户登录密码
</code></pre><p>安全漏洞：第二个passwd含义是普通用户切换到root用户，然后执行passwd修改密码，因此，普通用户也可以修改root用户的密码了：</p>
<pre><code>$sudo passwd root    #可以修改root用户的密码
</code></pre><p>第二行passwd授权应该这么写：</p>
<pre><code>user1    ALL=/usr/bin/passwd    [A-Za-z]*,    !/usr/bin/passwd &quot;&quot;,    !/usr/bin/passwd root
</code></pre><p>其中</p>
<ul>
<li><code>/usr/bin/passwd    [A-Za-z]*</code>代表passwd的用户名一定要包含字母，[A-Za-z]<em>是正则表达式，</em>表示重复</li>
<li><code>!/usr/bin/passwd &quot;&quot;</code>代表不允许passwd不加用户名执行。因为执行<code>$sudo passwd</code>时已经是root用户，不加用户名回车表示修改本用户的密码，即root用户的密码。！代表取反，不允许命令</li>
<li><code>!/usr/bin/passwd root</code>不允许修改root用户密码</li>
<li>以上三段代码中间有空格，顺序不能变</li>
</ul>
<h2 id="实例3-【危险】让普通用户sudo执行vi命令！"><a href="#实例3-【危险】让普通用户sudo执行vi命令！" class="headerlink" title="实例3 【危险】让普通用户sudo执行vi命令！"></a>实例3 【危险】让普通用户sudo执行vi命令！</h2><pre><code>user1 ALL=/bin/vi
</code></pre><p>危险：普通用户在执行sudo vi 命令的时，会切换到root用户，因此普通用户可以修改任意系统文件了。比如/etc/shadow是保存密码的文件，权限是000，普通用户无权限查看和编辑</p>
<pre><code>----------.  1 root root 1283 6月  15 21:32 shadow
</code></pre><p>但如果让普通用户可以sudo执行vi命令，那么就可以查看和修改所有密码了</p>
<pre><code>sudo vi /etc/shadow
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[centOS安装Oracle JDK替换OpenJDK详解]]></title>
      <url>http://shengdeng.github.io/2016/06/16/JDK%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p><strong>主要内容：</strong></p>
<p>在<strong>不卸载openJDK</strong>的情况下，centOS上解决oracle和openJDK的Java命令冲突的安装方法。以及openJDK为什么不能卸载？因为系统应用会依赖它。</p>
<a id="more"></a>
<h1 id="1-在centOS上安装JDK"><a href="#1-在centOS上安装JDK" class="headerlink" title="1. 在centOS上安装JDK"></a>1. 在centOS上安装JDK</h1><h2 id="1-1-快速步骤"><a href="#1-1-快速步骤" class="headerlink" title="1.1 快速步骤"></a>1.1 快速步骤</h2><p>centOS上按照以下步骤安装即可</p>
<p>第一步：到oracle官网下载tar包</p>
<p>第二步：解压tar包</p>
<pre><code>[stefan@localhost 下载]$ sudo tar -zxvf jdk-8u91-linux-x64.tar.gz -C /opt/
</code></pre><p>第三步：配置环境变量</p>
<pre><code>[stefan@localhost jdk1.8.0_91]$ sudo vim /etc/profile
</code></pre><p>在配置文件的最后添加几行，注意新的路径$JAVA_HOME/bin一定要放在老的路径$PATH前面：</p>
<pre><code># 自定义添加环境变量
export JAVA_HOME=/opt/jdk1.8.0_91
export PATH=$JAVA_HOME/bin:$PATH
</code></pre><p>第四步：生效</p>
<pre><code>[stefan@localhost jdk1.8.0_91]$ source /etc/profile
</code></pre><p>查看java的版本</p>
<pre><code>[stefan@localhost jdk1.8.0_91]$ java -version
java version &quot;1.8.0_91&quot;
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)
</code></pre><h2 id="1-2-安装详解"><a href="#1-2-安装详解" class="headerlink" title="1.2 安装详解"></a>1.2 安装详解</h2><h3 id="第一步：检查现有JDK版本；下载tar包"><a href="#第一步：检查现有JDK版本；下载tar包" class="headerlink" title="第一步：检查现有JDK版本；下载tar包"></a>第一步：检查现有JDK版本；下载tar包</h3><p>一般装系统的时候会自动安装openJDK：</p>
<pre><code>[stefan@localhost opt]$ java -version
openjdk version &quot;1.8.0_65&quot;
OpenJDK Runtime Environment (build 1.8.0_65-b17)
OpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode)
</code></pre><p>不需要删除这个版本，接下来安装的oracle的JDK的所有文件都在一个文件夹下，不会对系统rpm包安装的openJDK有影响，他们的安装位置不同，开源应用软件一般有关联JDK路径的设置，所以安装位置不同就不会冲突（详见RPM包安装软件的博文）。</p>
<blockquote>
<p><strong>为什么系统自带openJDK？</strong></p>
<p>系统很多应用软件需要JDK环境，众所周知，由于Oracle公司的JDK的版权问题，大多数Linux发行版在安装时都会附带开源的无版权问题的OpenJDK，以支持某些java程序软件。但是如果我们在Linux下做一些java语言的开发，很可能还是需要安装和使用Oracle公司的原版JDK（以下简称“JDK”）。</p>
<p><strong>为什么openJDK不能随便删除？</strong></p>
<p>如果卸载删除系统自带的openjdk，安装它的依赖也会被删除，又会引起其它一些软件的依赖性问题，即如果卸载openjdk需要连同卸载其它所有依赖于openjdk的软件，例如OpenOffice/LibreOffice。这样子做是不可能滴。所以我们只能在不删除openjdk的情况下安装jdk。</p>
<p>Tips:系统自带的安装软件不要随意删除。</p>
</blockquote>
<p>系统没有安装oracle的JDK，需要去<a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="external">oracle官网下载</a>，选择.tar.gz的归档包。</p>
<h3 id="第二步：解压tar包到安装目录"><a href="#第二步：解压tar包到安装目录" class="headerlink" title="第二步：解压tar包到安装目录"></a>第二步：解压tar包到安装目录</h3><p>/opt/为大型软件的安装目录，/usr/local/为应用软件的安装目录，我的大数据软件一般都放在/opt/下</p>
<pre><code>[stefan@localhost 下载]$ sudo tar -zxvf jdk-8u91-linux-x64.tar.gz -C /opt/
</code></pre><p>进入安装目录中</p>
<pre><code>[stefan@localhost 下载]$ cd /opt/jdk1.8.0_91/
[stefan@localhost jdk1.8.0_91]$ ll
总用量 25912
drwxr-xr-x. 2 10 143 4096 4月   1 16:07 bin
-r--r--r--. 1 10 143 3244 4月   1 16:06 COPYRIGHT
drwxr-xr-x. 4 10 143 4096 4月   1 16:06 db
drwxr-xr-x. 3 10 143 4096 4月   1 16:06 include
-rwxr-xr-x. 1 10 143  5092228 4月   1 11:32 javafx-src.zip
drwxr-xr-x. 5 10 143 4096 4月   1 16:07 jre
drwxr-xr-x. 5 10 143 4096 4月   1 16:07 lib
-r--r--r--. 1 10 143   40 4月   1 16:06 LICENSE
drwxr-xr-x. 4 10 143 4096 4月   1 16:06 man
-r--r--r--. 1 10 143  159 4月   1 16:06 README.html
-rw-r--r--. 1 10 143  525 4月   1 16:06 release
-rw-r--r--. 1 10 143 21103627 4月   1 16:06 src.zip
-rwxr-xr-x. 1 10 143   110114 4月   1 11:32 THIRDPARTYLICENSEREADME-JAVAFX.txt
-r--r--r--. 1 10 143   177094 4月   1 16:06 THIRDPARTYLICENSEREADME.txt
</code></pre><h4 id="jdk里有什么？"><a href="#jdk里有什么？" class="headerlink" title="jdk里有什么？"></a>jdk里有什么？</h4><ul>
<li><strong>\bin</strong> : <strong>使用java的所有命令</strong>，比如javac、java</li>
<li>\demo、\sample ： java的例子</li>
<li>\include ： 放c语言程序，因为jdk有些用到c语言</li>
<li><strong>\jre</strong> : java运行时环境</li>
<li>\lib : java运行时所需包文件</li>
<li>src.zip ： 放了一部分java源文件</li>
</ul>
<h3 id="第三步：设置环境变量"><a href="#第三步：设置环境变量" class="headerlink" title="第三步：设置环境变量"></a>第三步：设置环境变量</h3><pre><code>[stefan@localhost jdk1.8.0_91]$ sudo vim /etc/profile
</code></pre><p>在配置文件的最后添加几行，注意新的路径$JAVA_HOME/bin一定要放在老的路径$PATH前面：</p>
<pre><code># 自定义添加环境变量
export JAVA_HOME=/opt/jdk1.8.0_91
export PATH=$JAVA_HOME/bin:$PATH
</code></pre><p>这样做的效果可以查看PATH的值，显然oracleJDK的路径在最前面</p>
<pre><code>[stefan@localhost jdk1.8.0_91]$ echo $PATH
/opt/jdk1.8.0_91/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/stefan/.local/bin:/home/stefan/bin:/opt/jdk1.8.0_91/bin:/opt/jdk1.8.0_91/bin:/opt/jdk1.8.0_91/bin
</code></pre><h3 id="第四步：使配置生效"><a href="#第四步：使配置生效" class="headerlink" title="第四步：使配置生效"></a>第四步：使配置生效</h3><pre><code>[stefan@localhost jdk1.8.0_91]$ source /etc/profile
</code></pre><p>查看java的版本</p>
<pre><code>[stefan@localhost jdk1.8.0_91]$ java -version
java version &quot;1.8.0_91&quot;
Java(TM) SE Runtime Environment (build 1.8.0_91-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)
</code></pre><p>ok安装完成</p>
<h2 id="1-3-PATH在-JAVA-HOME-bin前面什么效果？"><a href="#1-3-PATH在-JAVA-HOME-bin前面什么效果？" class="headerlink" title="1.3 $PATH在$JAVA_HOME/bin前面什么效果？"></a>1.3 $PATH在$JAVA_HOME/bin前面什么效果？</h2><p>若第三步将$PATH在$JAVA_HOME/bin前</p>
<pre><code># 自定义添加环境变量
export JAVA_HOME=/opt/jdk1.8.0_91
export PATH=$PATH:$JAVA_HOME/bin
</code></pre><p>source生效配置文件以后，java的版本仍是openJDK，这是因为openJDK的安装路径为</p>
<pre><code>[stefan@localhost jdk1.8.0_91]$ whereis java
java: /usr/bin/java /usr/lib/java /etc/java /usr/share/java /opt/jdk1.8.0_91/bin/java /usr/share/man/man1/java.1.gz
</code></pre><p>/usr/bin/java中的java命令会先被找到并执行，这里面安装的是openJDK；而/opt/jdk1.8.0_91/bin/java里的oracle版java将不会执行。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[scala安装配置]]></title>
      <url>http://shengdeng.github.io/2016/06/16/scala%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url>
      <content type="html"></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux软件安装管理（四）：源码包安装]]></title>
      <url>http://shengdeng.github.io/2016/06/14/Linux%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E7%AE%A1%E7%90%86%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E6%BA%90%E7%A0%81%E5%8C%85%E5%AE%89%E8%A3%85/</url>
      <content type="html"><![CDATA[<h1 id="1-源码包与RPM包的区别"><a href="#1-源码包与RPM包的区别" class="headerlink" title="1.源码包与RPM包的区别"></a>1.源码包与RPM包的区别</h1><ul>
<li>安装之前的区别：概念上的区别<ul>
<li>详见（一）</li>
</ul>
</li>
<li>安装之后的区别：安装的位置不同<ul>
<li>重点：安装位置不同对软件使用有什么影响？</li>
</ul>
</li>
</ul>
<h2 id="1-1-RPM包的安装位置：默认位置"><a href="#1-1-RPM包的安装位置：默认位置" class="headerlink" title="1.1 RPM包的安装位置：默认位置"></a>1.1 RPM包的安装位置：默认位置</h2><p>回顾一下RPM包的默认安装位置</p>
<table>
<thead>
<tr>
<th>PRM包默认的安装位置</th>
<th>文件归类</th>
</tr>
</thead>
<tbody>
<tr>
<td>/etc/</td>
<td>配置文件安装目录</td>
</tr>
<tr>
<td>/usr/bin/</td>
<td>可执行的命令安装目录</td>
</tr>
<tr>
<td>/usr/lib/</td>
<td>程序所使用的函数库保存位置</td>
</tr>
<tr>
<td>/usr/share/doc/</td>
<td>基本的软件使用手册保存位置</td>
</tr>
<tr>
<td>/usr/share/man/</td>
<td>帮助文件保存位置</td>
</tr>
</tbody>
</table>
<p>以上表格只是一个一般情况，具体RPM包软件的安装位置是写这个软件的人决定的，安装的时候无需指定安装位置。但是，rpm安装支持指定安装位置</p>
<pre><code>rpm --help
</code></pre><p>可以找到有安装路径的参数</p>
<pre><code>--prefix=&lt;dir&gt;   如果可重定位，便把软件包重定位到 &lt;dir&gt;
</code></pre><ul>
<li>rpm包安装通常不指定安装位置，其安装的默认位置是系统设定应该安装的位置；</li>
<li>因为软件安装的默认位置“到处都是”，所以提供了<strong>卸载命令一次删除</strong> <code>rpm -e 包名</code></li>
<li>若自定义安装位置，系统反而找不到，因此<strong>RPM包不建议自己指定安装路径</strong>。例如<ul>
<li>RPM包安装的服务可以使用系统服务管理命令service来管理。例如RPM包安装的apache的启动方法有两种<ul>
<li>根据绝对路径启动：<code>/etc/rc.d/init.d/httpd start</code></li>
<li>系统命令service启动：<code>service http start</code></li>
</ul>
</li>
<li>如果自己指定了安装位置，安装路径就不是<code>/etc/rc.d/init.d/httpd</code>了，以上两个启动命令都用不了！！因为service命令也是要先搜索默认路径/etc/rc.d/init.d/httpd</li>
</ul>
</li>
</ul>
<p><strong>总结：RPM包安装就用默认安装位置，好处是配置文件就在/etc下，启动程序就在/usr/bin或者/usr/sbin下，管理方便，符合系统要求，还有卸载命令</strong><code>rpm -e 包名</code>。</p>
<h2 id="1-2-源码包的安装位置-手工指定"><a href="#1-2-源码包的安装位置-手工指定" class="headerlink" title="1.2 源码包的安装位置:手工指定"></a>1.2 源码包的安装位置:手工指定</h2><ul>
<li>安装在指定的位置中，一般是：/usr/local/软件名/<ul>
<li>local相当于win下的program file</li>
<li>源码包为什么一定要指定安装目录？因为源码包安装没有卸载命令。若不指定，软件也会装的到处都是，删也删不干净</li>
<li><strong>源码包卸载方式：包安装目录直接删除即可</strong></li>
</ul>
</li>
</ul>
<h2 id="1-3-安装位置不同带来什么后果"><a href="#1-3-安装位置不同带来什么后果" class="headerlink" title="1.3 安装位置不同带来什么后果"></a>1.3 安装位置不同带来什么后果</h2><p>rpm采用默认位置安装，源码包采用自定义位置安装。导致软件管理的不同：</p>
<ul>
<li>启动方法不同：<ul>
<li>所有的rpm服务可以用service启动。因为rpm软件的启动程序都会装在<code>/etc/rc.d/init.d/软件名</code>目录中。源码包则不能用service。</li>
<li>源码包安装的服务的启动：<code>/usr/local/软件名/bin/启动文件 start</code></li>
</ul>
</li>
<li>卸载方法不同：<ul>
<li>rpm用命令；源码包直接删除安装位置文件夹</li>
</ul>
</li>
</ul>
<blockquote>
<p>已经用yum安装了一个rpm包软件A，用源码包再安一个软件A？</p>
<p>是可以得。因为两种安装方法安装位置不一样。但是，两个软件同时只能启动一个，因为他们会占用相同的资源，比如端口。在实际的生产环境，绝不会装两个相同的软件，因为只能用一个，还浪费资源。</p>
</blockquote>
<h2 id="1-4-应该选择哪种安装方式？"><a href="#1-4-应该选择哪种安装方式？" class="headerlink" title="1.4 应该选择哪种安装方式？"></a>1.4 应该选择哪种安装方式？</h2><ul>
<li>源码包开源自定义本机编译效率更高。如果软件是用来给成千上万的客户端访问的，请用源码包安装，效率更高</li>
<li>如果是底层软件，像编译软件gcc，用rpm包更简单，不会有过多用户访问，效率问题不明显</li>
</ul>
<h1 id="2-源码包安装"><a href="#2-源码包安装" class="headerlink" title="2. 源码包安装"></a>2. 源码包安装</h1><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul>
<li>源码包保存位置：/usr/local/scr<ul>
<li>这个位置是系统预留的，装好系统就有，专门用来保存源代码的文件夹</li>
</ul>
</li>
<li>软件安装位置：/usr/local</li>
<li>如何确定安装过程报错<ul>
<li>安装过程停止</li>
<li>并出现error、warning或no的提示</li>
</ul>
</li>
<li>安装的时候信息量大，不需要每行去看，只要确认没有报错就行</li>
</ul>
<p>一般源码包中会有一个INSTALLATION的安装说明</p>
<pre><code>./configure --prefix=安装路径/usr/local/软件名
make
make install
/usr/local/软件名/bin/启动文件 start
</code></pre>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[VMware中网络连接的配置]]></title>
      <url>http://shengdeng.github.io/2016/06/13/VMware%E4%B8%AD%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<p>VMware中虚拟机联网方式：桥接模式、host-only模式、NAT模式使用介绍。<br><a id="more"></a></p>
<h1 id="1-虚拟机三种网络设置功能"><a href="#1-虚拟机三种网络设置功能" class="headerlink" title="1.虚拟机三种网络设置功能"></a>1.虚拟机三种网络设置功能</h1><p>在windous上通过<code>控制面板→更改适配器设置</code>可以看到</p>
<ul>
<li>计算机有两个<strong>真实网卡</strong>：有线连接和无线连接；</li>
<li>安装完虚拟机以后，会出现两块<strong>虚拟网卡</strong>VMnet1和VMnet8。</li>
</ul>
<p><img src="http://7xtorv.com1.z0.glb.clouddn.com/3333.PNG" alt=""></p>
<p>VMware中网络适配器的三种设置：</p>
<ul>
<li><strong>桥接方式</strong><ul>
<li>如果选择桥接，虚拟机<strong>使用真实的网卡</strong>，即本地的有线连接或者本地的无线连接。将<strong>虚拟机的IP设置成真实机IP同一个网段</strong>，就可以与真实机之间进行通信，还可以与局域网（同网段）其他计算机进行通信。</li>
<li>坏处：会占用网段的一个ip，如果有多台虚拟机使用桥接，占用很多ip，会出现ip地址冲突现象。</li>
</ul>
</li>
<li><strong>NAT模式</strong><ul>
<li>虚拟机通过VMware8这块假网卡与真实机进行通信</li>
</ul>
</li>
<li><strong>Host-Only模式</strong><ul>
<li>虚拟机通过VMware1这块假网卡与真实机通信</li>
</ul>
</li>
</ul>
<p>区别一：</p>
<ul>
<li>桥接：利用真实网卡，需设置与真实机<strong>在同一个网段</strong>即局域网内，可与局域网内计算机通信</li>
<li>Nat\Host-only利用虚拟网卡，只<strong>能与真实机通信</strong>，但和真实机所在的局域网不能通信，因为<strong>不在同一网段</strong></li>
</ul>
<p>区别二：</p>
<ul>
<li>host-only：只能和真实机通信</li>
<li>NAT：虚拟机和真实机、<strong>互联网</strong>通信</li>
</ul>
<h1 id="2-host-only模式ip设置"><a href="#2-host-only模式ip设置" class="headerlink" title="2.host-only模式ip设置"></a>2.host-only模式ip设置</h1><p>第一步：选中host-only </p>
<p>第二步：查看VMware1的TCP/IPV4网络属性，查看这块虚拟网卡的网段；由于<strong>真实机和虚拟机通过VMware1网卡通信</strong>，所以虚拟机IP要在VMware网卡网段中。</p>
<p>第三步：设置虚拟机ip，使得ip与VMware1在同一网段。</p>
<pre><code>ifconfig #查看当前网卡信息（if，interface接口）
</code></pre><p>ip的信息显示在<code>eth0：</code>内容里，eth0指的是一块网卡ethernet以太网；lo是回环网卡loopback，用于在没有网卡的时候，本机通信测试，不需要插网线网卡自己ping自己，所有的操作系统OS都有，没有多大实际意义。</p>
<h1 id="3-桥接模式ip设置"><a href="#3-桥接模式ip设置" class="headerlink" title="3.桥接模式ip设置"></a>3.桥接模式ip设置</h1><p>设置虚拟机的ip与真实的网卡网段一致。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux软件安装管理（三）：yum在线安装]]></title>
      <url>http://shengdeng.github.io/2016/06/13/Linux%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Ayum%E5%9C%A8%E7%BA%BF%E5%AE%89%E8%A3%85/</url>
      <content type="html"><![CDATA[<ol>
<li>yum仓库的配置文件详解和仓库的创建</li>
<li>yum的安装软件命令</li>
<li>升级、卸载(依赖也会卸载掉)使用需谨慎；yum查询仓库中的软件，其他rpm包查询推荐用rpm查询命令。</li>
<li>yum软件组的查询与安装，装语言包很方便</li>
</ol>
<a id="more"></a>
<p>rpm软件包形式管理软件，需要手工解决软件的依赖关系，rpm安装一个软件时有时需要先安装其他软件，十分麻烦。yum可以解决这个问题，将所有软件包放到官方服务器上，当进行yum在线安装时，可以自动解决依赖性的问题。redhat的yum在线安装需要付费，而centos是免费的。</p>
<h1 id="1-yum仓库"><a href="#1-yum仓库" class="headerlink" title="1. yum仓库"></a>1. yum仓库</h1><p>yum引入仓库repo的概念，仓库用来存放所有现有的rpm包，当使用yum安装一个rpm，如果存在依赖关系，<strong>会自动在仓库中查找依赖软件并安装</strong>。如果仓库中没有这个依赖，那么yum安装失败。</p>
<p>仓库可以是<strong>本地的（文件夹）</strong>，也可以通过HTTP、FTP、NFS形式使用集中的、统一的<strong>网络仓库</strong>。</p>
<p>所以，要使用yum，必须先配置yum仓库：</p>
<h2 id="1-1-yum仓库repo的配置"><a href="#1-1-yum仓库repo的配置" class="headerlink" title="1.1 yum仓库repo的配置"></a>1.1 yum仓库repo的配置</h2><ul>
<li>yum仓库的配置文件存放在<strong>/etc/yum.repos.d/</strong></li>
<li>仓库配置文件名随意取，但一定要<strong>以.repo结尾</strong></li>
<li><strong>一个.repo配置文件</strong>中可以保存<strong>多个仓库的配置信息</strong></li>
<li>centOS安装好以后，yum仓库已经配置好了，因为是免费的。如果使用红帽，默认不会配置，可以去购买红帽的订阅，或者上网查仓库的地址，或者使用centOS的网络仓库也可以。</li>
</ul>
<p>例如，CentOS-Base.repo配置文件中有很多仓库，其中一个名为base仓库的配置：</p>
<pre><code>[base]
name=CentOS-$releasever - Base
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os&amp;infra=$infra
#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
</code></pre><p>注意：等号=无空格。<code>file:///etc/</code>中<code>file://</code>是协议的双斜杠，路径是<code>/etc/</code>，所以有三个双斜杠。</p>
<p>配置参数说明：</p>
<ul>
<li><strong>[base]</strong> 仓库的名字，用[]扩起，随意取名</li>
<li><strong>name=</strong>this is 说明，可随便写</li>
<li><strong>mirrorlist=</strong> 镜像站点，可以注释掉</li>
<li><strong>baseurl=</strong> yum仓库的地址。可以是本地的file：//。也可以是网络仓库http、ftp、nfs</li>
<li><strong>enabled</strong> 仓库是否生效<ul>
<li>enable=1或者<strong>不写</strong>：生效</li>
<li>enable=0：不生效</li>
</ul>
</li>
<li><strong>gpgcheck</strong> RPM数字证书是否生效<ul>
<li>gpgcheck=1：生效。建议开启，可以验证下载的软件是否安全完整</li>
<li>gpgcheck=0：不生效</li>
</ul>
</li>
<li><strong>gpgkey</strong> 数字证书的公钥文件保存位置/etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7，不用修改</li>
</ul>
<h2 id="1-2-创建yum仓库"><a href="#1-2-创建yum仓库" class="headerlink" title="1.2 创建yum仓库"></a>1.2 创建yum仓库</h2><h3 id="第一步：将所有RPM拷贝到一个文件夹中"><a href="#第一步：将所有RPM拷贝到一个文件夹中" class="headerlink" title="第一步：将所有RPM拷贝到一个文件夹中"></a>第一步：将所有RPM拷贝到一个文件夹中</h3><p>如将光盘中的rpm包放到本地的文件夹中</p>
<ol>
<li>挂载光盘<ul>
<li>mkdir /mnt/cdrom #建立挂载点，即文件夹</li>
<li>mount /dev/cdrom /mnt/cdrom #挂载光盘</li>
</ul>
</li>
<li>复制光盘中的rpm包到本地文件夹中<ul>
<li>mkdir my_yum</li>
<li>cp -rv /mnt/cdrom/package/* /my_yum </li>
</ul>
</li>
</ol>
<h3 id="第二步：创建索引文件"><a href="#第二步：创建索引文件" class="headerlink" title="第二步：创建索引文件"></a>第二步：创建索引文件</h3><ol>
<li>先用rpm命令手工安装creatrepo软件，creatrepo用于创建索引文件<ul>
<li>cd /my_yum #进入仓库的文件夹</li>
<li>rpm -ivh creatrepo #会报错，提示安装deltarpm,phython-deltarpm</li>
<li>rpm -ivh creatrepo deltarpm phython-deltarpm #如果需要解决依赖，可以三个一起安装</li>
</ul>
</li>
<li>使用creatrepo创建索引文件<ul>
<li>creatrepo -v /yum仓库的路径</li>
<li>比如，creatrepo -v /my_yum</li>
<li>命令会对仓库中的rpm包创建索引，会在当前目录（仓库文件夹下）创建一个repodata的目录，里面保存索引信息</li>
</ul>
</li>
</ol>
<h3 id="第三步：在-etc-yum-repos-d-中添加仓库的配置文件"><a href="#第三步：在-etc-yum-repos-d-中添加仓库的配置文件" class="headerlink" title="第三步：在/etc/yum.repos.d/中添加仓库的配置文件"></a>第三步：在/etc/yum.repos.d/中添加仓库的配置文件</h3><ol>
<li>cd /etc/yum.repos.d/</li>
<li><p>vim myyum.repo</p>
<p> [myyum]<br> name=this is my repo<br> baseurl=file:///my_yum<br> enable=1 #也可以不写默认开启<br> gpgcheck=1 #自己创建的仓库，也可以不验证gpgcheck=0<br> gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</p>
</li>
</ol>
<p>注意：yum为了提高速度，会缓存很多信息，但也会导致故障。所以每次使用yum命令后，请清除缓存：</p>
<pre><code>yum clean call
</code></pre><h1 id="2-yum命令"><a href="#2-yum命令" class="headerlink" title="2.yum命令"></a>2.yum命令</h1><h2 id="2-1-查询"><a href="#2-1-查询" class="headerlink" title="2.1 查询"></a>2.1 查询</h2><p>注意：关于rpm包的查询，用rpm查询命令比较好。</p>
<h3 id="查询仓库的软件"><a href="#查询仓库的软件" class="headerlink" title="查询仓库的软件"></a>查询仓库的软件</h3><ul>
<li><p>yum list</p>
<ul>
<li>查询<strong>仓库中所有</strong>可用<strong>软件包</strong>列表</li>
<li>三列：软件名|版本号|来自哪个仓库</li>
</ul>
</li>
<li><p>yum list (all|installed|recent|updates)</p>
<ul>
<li>列出（全部|已安装的|最近安装的|有更新的）软件</li>
</ul>
</li>
<li><p>yum search 关键字</p>
<ul>
<li>搜索仓库中所有和关键字相关的包</li>
</ul>
</li>
</ul>
<p>注意：yum没有本机装了哪些rpm包的命令；yum没有查询rpm包装到了哪些位置的命令。</p>
<h3 id="查询软件详细信息"><a href="#查询软件详细信息" class="headerlink" title="查询软件详细信息"></a>查询软件详细信息</h3><ul>
<li>yum info 软件名 <ul>
<li>列出软件的<strong>详细信息</strong></li>
<li>可以用tab补全</li>
</ul>
</li>
</ul>
<h3 id="查询文件属于哪个rpm包"><a href="#查询文件属于哪个rpm包" class="headerlink" title="查询文件属于哪个rpm包"></a>查询文件属于哪个rpm包</h3><ul>
<li>yum whatprovides 文件名</li>
</ul>
<h2 id="2-2-安装"><a href="#2-2-安装" class="headerlink" title="2.2 安装"></a>2.2 安装</h2><ul>
<li>yum -y install 包名<ul>
<li>install 安装</li>
<li>-y 自动回答yes</li>
</ul>
</li>
</ul>
<p>rpm命令需要区分包全名和包名；yum命令不需要包全名，只要写包名即可。</p>
<p>系统最小化安装，安装的软件就越少，需要解决的依赖就越多。比如要安装c的编译器gcc，必须使用rpm安装，不能用源码包安装方法，因为源码不能编译。</p>
<h2 id="2-3-升级"><a href="#2-3-升级" class="headerlink" title="2.3 升级"></a>2.3 升级</h2><ul>
<li>yum -y update 包名<ul>
<li>update</li>
<li>-y 自动回答yes</li>
</ul>
</li>
</ul>
<blockquote>
<p>危险：对于服务器来说，最好不要升级。除非有重要bug。不要写成<code>yum -y update</code>，会导致内核和所有程序的更新，可能会导致系统直接崩溃。</p>
</blockquote>
<h2 id="2-4-卸载"><a href="#2-4-卸载" class="headerlink" title="2.4 卸载"></a>2.4 卸载</h2><ul>
<li>yum -y remove 包名<ul>
<li>remove</li>
<li>-y 自动回答yes</li>
</ul>
</li>
</ul>
<blockquote>
<p>危险：服务器最好使用最小化安装，用什么软件安装什么软件，<strong>尽量不要卸载</strong>！！因为，卸载时也会将<strong>依赖的软件依次卸载</strong>，而且yum不会询问直接卸载。如果软件依赖的包被系统依赖，却被卸载了，可能导致崩溃！！</p>
<p>所以，最小化安装！用什么装什么！不要卸载！服务器上的操作一定要小心，管理员最好把命令都记下来作为日志，都是为了防止服务器 崩溃。</p>
</blockquote>
<h2 id="2-5-yum软件组管理命令"><a href="#2-5-yum软件组管理命令" class="headerlink" title="2.5 yum软件组管理命令"></a>2.5 yum软件组管理命令</h2><p>例如，如果linux安装的时候没有选择中文语言，不能正常显示中文，  如果要手工安装中文语言很麻烦，因为他需要装很多个包，包括字体包编码包，但是他们都属于<code>中文支持[zh]</code>组包内，使用yum grouplist就能查看到所有组包：</p>
<ul>
<li><p>yum grouplist </p>
<ul>
<li><strong>列出</strong>所有可用的软件组列表</li>
<li><strong>本机</strong>使用grouplist命令时组名是英文显示；<strong>远程登录</strong>时组名默认中文显示</li>
<li>要让<strong>远程登陆</strong>也显示英文，修改语言即可,执行命令：<code>LANG=en_US</code>；改回来：<code>LANG=zh_CN.utf8</code>(临时生效，永久改变语言要改/etc下的下相应配置文件)</li>
</ul>
</li>
<li><p>yum groupinstall 软件组名(英文)</p>
<ul>
<li>安装指定的软件组</li>
<li>组名可以有grouplist查询出来</li>
<li>软件组名必须是英文</li>
</ul>
</li>
<li><p>yum groupremove 软件组名(英文)</p>
<ul>
<li>卸载指定的软件组</li>
<li>同样不推荐随便卸载</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux软件安装管理（二）：RPM命令管理]]></title>
      <url>http://shengdeng.github.io/2016/06/12/Linux%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E7%AE%A1%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89-RPM%E5%91%BD%E4%BB%A4%E7%AE%A1%E7%90%86/</url>
      <content type="html"><![CDATA[<p><strong>内容提要</strong></p>
<ol>
<li>RPM包都在系统安装光盘中</li>
<li>rpm包命名规则：httpd-2.2.15-15.el6.centos.1.i686.rpm</li>
<li>rpm包的依赖性问题</li>
<li>rpm包命令：安装-i;卸载-e;升级-U；查询-q,-qa,-qi,-qip,-ql,-qlp,-qf;校验 -V；rpm中提取文件。其中，安装卸载可以被yum取代。而yum没有查询、校验rpm包的功能</li>
</ol>
<a id="more"></a>
<h1 id="1-RPM包的来源"><a href="#1-RPM包的来源" class="headerlink" title="1.RPM包的来源"></a>1.RPM包的来源</h1><p>Linux的系统光盘DVD1中ISO中有软件的RPM包，只需在虚拟机<strong>光驱</strong>中选择这个DVD1，勾选<strong>连接</strong>选项，即给光驱通电。</p>
<p>给虚拟机光驱中加入DVD后，在linux系统中给光盘创建一个挂载点</p>
<pre><code>mkdir /mnt/cdrom
</code></pre><p>输入挂载命令</p>
<pre><code>mount /dev/sr0 /mnt/cdrom
</code></pre><p>然后打开文件夹<code>cd /mnt/cdrom</code>，再<code>cd package/</code>目录中可以看到好几千个RPM包。所有的RPM包在系统光盘中的package目录中。</p>
<h1 id="2-RPM包命名规则"><a href="#2-RPM包命名规则" class="headerlink" title="2.RPM包命名规则"></a>2.RPM包命名规则</h1><p>所有的RPM包都必须按照以下规则命名，看懂它有助于选择合适的软件包：</p>
<ul>
<li>httpd-2.2.15-15.el6.centos.1.i686.rpm<ul>
<li>包名称，即软件名称httpd</li>
<li>版本号2.2.15</li>
<li>软件发布次数15</li>
<li>适合的Linux平台el6.centos；el6指的是redhat的企业版</li>
<li>适合的硬件平台i686；如果是x64表示只适合64位系统</li>
<li>扩展名rpm</li>
</ul>
</li>
</ul>
<p>可见，rpm是<strong>针对特定平台</strong>编译的二进制包！rpm支持不同平台指的是针对不同平台发布不同的rpm。</p>
<h1 id="3-RPM包的依赖性"><a href="#3-RPM包的依赖性" class="headerlink" title="3.RPM包的依赖性"></a>3.RPM包的依赖性</h1><ul>
<li>树形依赖<ul>
<li>a→b→c，装a的时候需要装b,装b的时候需要装c</li>
<li>先装c,再装b,最后装a</li>
<li>依赖可以在光盘中找到，光盘中找依赖（package目录下自动补全）软后安装</li>
</ul>
</li>
<li>环形依赖<ul>
<li>a→b→c→a</li>
<li>a\b\c三个包放在一起安装</li>
<li>依赖可以在光盘中找到，光盘中找依赖（package目录下自动补全）软后安装</li>
</ul>
</li>
<li>模块依赖<ul>
<li><strong>.so.数字</strong>结尾的库文件</li>
<li>在光盘中找不到，因为它是一个文件（函数），而不是软件包</li>
<li>它其实是藏身在光盘中几千个RPM包的某一个包中的文件而已，只要把这个文件所在的包装上就行</li>
<li>这个文件在哪个包中？登陆<a href="http://www.rpmfind.net/" target="_blank" rel="external">www.rpmfind.net</a>中查询</li>
</ul>
</li>
</ul>
<p>报错提示：B依赖A，需要先装A</p>
<blockquote>
<p>error：Failed dependencise: A is needed by B</p>
</blockquote>
<p>yum在线安装可以解决rpm依赖性的问题，yum仓库里放了所有的软件在服务器上，只要能够访问服务器，使用yum命令安装，所有的依赖会自动解决。但是redhad将yum服务作为付费项目，centos的yum是免费的。</p>
<h1 id="4-rpm包管理命令"><a href="#4-rpm包管理命令" class="headerlink" title="4.rpm包管理命令"></a>4.rpm包管理命令</h1><p>安装和卸载时使用的包（全）名不同，注意区别：</p>
<ul>
<li><strong>包全名</strong>：操作的包是<strong>没有安装的软件包</strong>时，使用包全名，<strong>包括路径</strong><ul>
<li>即此时包只是一个文件</li>
<li>比如<strong>安装包时、升级包时</strong>到这个文件的目录下或者用绝对路径+包名</li>
</ul>
</li>
<li><strong>包名</strong>：操作<strong>已安装的软件包</strong>时，使用包名，即httpd<ul>
<li>比如<strong>卸载时、查询时</strong>，直接使用包名即可</li>
<li>安装后的软件，软件名都会保存在/var/lib/rpm的数据库中</li>
</ul>
</li>
</ul>
<h2 id="4-1-rpm安装命令-i"><a href="#4-1-rpm安装命令-i" class="headerlink" title="4.1 rpm安装命令 -i"></a>4.1 rpm安装命令 -i</h2><ul>
<li><code>rpm -ivh 包全名</code></li>
<li>选项<ul>
<li>-i（install）</li>
<li>-v （verbose）显示详细信息</li>
<li>-h (hash)显示进度</li>
<li>–nodeps 不检测依赖性，<strong>不推荐使用</strong>，因为不解决依赖软件安了也用不了</li>
</ul>
</li>
</ul>
<p>提示：安装的时候，<strong>会告诉你要依赖哪些rpm，依次装完这些软件即可</strong>；安装成功会提示两个100%，第一个是准备成功，第二个才是安装成功；rpm包进入光盘中的package中查找</p>
<p>虽然yum方便的解决了依赖，可以安装和卸载rpm包，但是<strong>yum没有查询和校验命令</strong>。今后可以用<strong>yum的安装和卸载命令取代rpm的安装和卸载</strong>，但是查询和校验依然用rpm的命令。</p>
<h2 id="4-2-rpm升级命令-U"><a href="#4-2-rpm升级命令-U" class="headerlink" title="4.2 rpm升级命令 -U"></a>4.2 rpm升级命令 -U</h2><ul>
<li><code>rpm -Uvh 包全名</code></li>
<li>选项<ul>
<li>-U（upgrade）升级</li>
<li>-v （verbose）显示详细信息</li>
<li>-h (hash)显示进度</li>
</ul>
</li>
</ul>
<p>注意:</p>
<ul>
<li>新的升级包对系统来说也是完全陌生的包，必须用包全名；</li>
<li>得有一个比当前版本更高版本的包，才会升级安装！比如安装了一个2.0版本的httpd软件，现在用<strong>更高版本的包</strong>httpd-2.2.15-15.el6.centos.1.i686.rpm进行升级安装</li>
<li>版本相同不会安装，报A is already installed；没有安装过，则相当于安装命名</li>
</ul>
<h2 id="4-3-rpm卸载命令-e"><a href="#4-3-rpm卸载命令-e" class="headerlink" title="4.3 rpm卸载命令 -e"></a>4.3 rpm卸载命令 -e</h2><ul>
<li><code>rpm -e 包名</code></li>
<li>选项<ul>
<li>-e（erase）卸载</li>
<li>–nodeps 不检测依赖性，<strong>不推荐使用</strong>，否则卸载不干净，依赖的软件都卸不掉！！</li>
</ul>
</li>
</ul>
<p>注意：</p>
<ul>
<li>使用包名即可</li>
<li>卸载也要解决依赖性，卸载时的依赖性与安装时相反，依次卸载</li>
</ul>
<blockquote>
<p><strong>rpm包为什么准备了卸载命令？</strong></p>
<p>在安装rpm包的时候，没有指定安装位置，软件将会安装在系统默认的位置，即写这个rpm包的人认为软件装在哪最好，就会装在哪。会在系统中装的乱七八糟，手工删除要一个一个文件删很麻烦，所以有卸载命令更方便。</p>
<p>但是源码包安装没有卸载命令，因为源码包指定安装位置，直接删除安装的目录即可。</p>
<p>linux比windows稳定是因为linux不会产生垃圾文件，windows卸载软件常卸载不干净注册表中还有很多垃圾文件；linux不需要考虑这些，删除安装的位置文件即可，不会遗留垃圾文件。</p>
</blockquote>
<h2 id="4-4-rpm包查询方法-q"><a href="#4-4-rpm包查询方法-q" class="headerlink" title="4.4 rpm包查询方法 -q"></a>4.4 rpm包查询方法 -q</h2><p>yum只能实现安装、卸载，但是不能<strong>查询已经安装好的rpm包</strong>。常用命令如下</p>
<h3 id="4-4-1-查询软件是否安装-q："><a href="#4-4-1-查询软件是否安装-q：" class="headerlink" title="4.4.1 查询软件是否安装 -q："></a>4.4.1 查询软件<strong>是否安装</strong> -q：</h3><ul>
<li><code>rpm -q 包名</code><ul>
<li>查询包是否<strong>安装</strong>，使用包名即可，返回rpm包的命名</li>
<li>-q (query) 查询</li>
</ul>
</li>
</ul>
<ul>
<li><code>rpm -qa</code><ul>
<li>查询所有<strong>已经安装的</strong>rpm包</li>
<li>-a (all)</li>
<li><code>rpm -qa | grep 关键字</code> 返回含有关键字的已安装的rpm包（√推荐使用）</li>
</ul>
</li>
</ul>
<h3 id="4-4-2-查询软件包的详细信息-i："><a href="#4-4-2-查询软件包的详细信息-i：" class="headerlink" title="4.4.2 查询软件包的详细信息 -i："></a>4.4.2 查询软件包的<strong>详细信息</strong> -i：</h3><ul>
<li><p><code>rpm -qi (已安装的)包名</code></p>
<ul>
<li>-i (information) 返回包的详细信息（写rpm包的作者写的详细信息）</li>
</ul>
</li>
<li><p><code>rpm -qip （未安装的）包全名</code></p>
<ul>
<li>-p (package) 查询未安装包信息</li>
</ul>
</li>
</ul>
<p>查询详细信息命令用的不多，但当软件出现问题时，可以通过查看URL官方网站去查找解决方法</p>
<h3 id="4-4-3-查询包中文件安装位置-l："><a href="#4-4-3-查询包中文件安装位置-l：" class="headerlink" title="4.4.3 查询包中文件安装位置 -l："></a>4.4.3 查询包中<strong>文件安装位置</strong> -l：</h3><ul>
<li><code>rpm -ql 包名</code><ul>
<li>-l (list) 列表</li>
<li>可以查看到<strong>软件的每个文件安装的详细位置</strong>，目录及其子目录；比如安装后的软件文件一般会分布在/etc配置文件中，/usr/lib函数库文件中，/var目录下</li>
<li>rpm包会将安装的到处都是，装在哪是写rpm包的人决定的，所以安装的乱七八糟</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>PRM包默认的安装位置</th>
<th>文件归类</th>
</tr>
</thead>
<tbody>
<tr>
<td>/etc/</td>
<td>配置文件安装目录</td>
</tr>
<tr>
<td>/usr/bin/</td>
<td>可执行的命令安装目录</td>
</tr>
<tr>
<td>/usr/lib/</td>
<td>程序所使用的函数库保存位置</td>
</tr>
<tr>
<td>/usr/share/doc/</td>
<td>基本的软件使用手册保存位置</td>
</tr>
<tr>
<td>/usr/share/man/</td>
<td>帮助文件保存位置</td>
</tr>
</tbody>
</table>
<ul>
<li><code>rpm -qlp 包全名</code><ul>
<li>-l (list) 列表</li>
<li>-p (package)查询未安装包的信息</li>
<li><strong>查看未安装的包</strong>安装以后将会安装到哪里</li>
</ul>
</li>
</ul>
<h3 id="4-4-4-查询系统文件属于哪个RPM包-f"><a href="#4-4-4-查询系统文件属于哪个RPM包-f" class="headerlink" title="4.4.4 查询系统文件属于哪个RPM包 -f"></a>4.4.4 查询系统文件属于哪个RPM包 -f</h3><ul>
<li><code>rpm -qf 系统文件名</code><ul>
<li>-f (file) 查询系统文件属于哪个软件包</li>
</ul>
</li>
</ul>
<p>Linux系统和几乎所有的系统软件都是rpm包安装的，例如查找配置文件yum.conf属于哪个包</p>
<pre><code>rpm -qf yum.conf
</code></pre><p>返回<code>yum-3.2.29-30.el6.centos.noarch</code>所属包名</p>
<p>但是，你自己创建的文件夹 <code>touch aaa</code>去查询<code>rpm -qf aaa</code>则会返回 <code>file /root/aaa is not owned by any package.</code></p>
<p>只有<strong>通过rpm包安装产生的文件</strong>才能反向查询属于哪个rpm包。</p>
<h3 id="4-4-5-查询软件包的依赖性-R"><a href="#4-4-5-查询软件包的依赖性-R" class="headerlink" title="4.4.5 查询软件包的依赖性 -R"></a>4.4.5 查询软件包的依赖性 -R</h3><ul>
<li><p><code>rpm -qR 包名</code></p>
<ul>
<li>-R (requires) 查询软件包的依赖性</li>
</ul>
</li>
<li><p><code>rpm -qRp 包全名</code></p>
<ul>
<li>-p (package) <strong>查看未安装包</strong>的依赖性</li>
<li>依赖包是在写这个rpm包的时候就依赖了，所以未安装也可以查询</li>
</ul>
</li>
</ul>
<p>查询出来的结果其实意义不大，结果中还包含了linux的标准shell即bash这样的依赖，所以返回的依赖会很多，可读性不强。还不如<strong>直接去装软件包</strong>，让他去报错误，提示还需要哪些依赖。</p>
<h2 id="4-5-rpm包的校验"><a href="#4-5-rpm包的校验" class="headerlink" title="4.5 rpm包的校验"></a>4.5 rpm包的校验</h2><ul>
<li>rpm -V 已安装的包名<ul>
<li>-V (verify)校验指定rpm包中的文件 </li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>验证内容中的8个信息</strong>的具体内容如下：SM5DLUGT</p>
<ol>
<li>S 文件<strong>大小</strong>是否改变</li>
<li>M 文件的<strong>类型</strong>或文件的<strong>权限(rwx)</strong>是否改变</li>
<li>5 文件的<strong>MD5校验和</strong>是否发生改变(可以看成文件内容是否改变)</li>
<li>D 设备的主从代码是否改变</li>
<li>L <strong>文件路径</strong>是否改变</li>
<li>U 文件的<strong>所属用户</strong>是否改变</li>
<li>G 文件的<strong>所属组</strong>是否改变</li>
<li>T 文件的<strong>修改时间</strong>是否改变</li>
</ol>
</blockquote>
<p><strong>MD5校验和的作用</strong>是，下载完一个软件以后，利用MD5校验软件去检验这个下载的程序，然后拿你检验出的MD5校验码和官网的MD5校验码比较，如果一样则证明<strong>文件的完整性</strong>。</p>
<blockquote>
<p><strong>文件类型</strong>校验：</p>
<ul>
<li>c config file 配置文件</li>
<li>d documentation 普通文档</li>
<li>g ghost 鬼文件，很少见，<strong>说明该文件不应该被这个RPM包包含</strong>，出现这个可能被攻击</li>
<li>L license file 授权文件</li>
<li>r read me 描述文件</li>
</ul>
</blockquote>
<p>如果安装包未改变，校验以后则不返回任何东西。如果有改变，则返回以上信息。例如，给httpd的配置文件httpd.conf中注释部分加一点内容</p>
<pre><code># rpm -V httpd
</code></pre><p>返回：</p>
<pre><code>S.5....T. c /etc/httpd/conf/httpd.conf
</code></pre><p>说明S\5\T这些项都被改变，c指的是一个正常的配置文件。如果自己从来没有动过这个文件，校验结果显示被修改了，可以怀疑有人偷偷修改过。</p>
<h2 id="4-6-rpm包中文件提取"><a href="#4-6-rpm包中文件提取" class="headerlink" title="4.6 rpm包中文件提取"></a>4.6 rpm包中文件提取</h2><p>只要指导rpm包中有哪些文件，就可以通过以下命令提取文件，包中文件绝对路径就是默认安装的位置，见 4.4.3</p>
<ul>
<li><code>rpm2cpio 包全名 | cpio -idv .包中文件绝对路径</code></li>
</ul>
<blockquote>
<p>两个命令的功能说明：</p>
<ul>
<li><p>rpm2cpio 包全名</p>
<ul>
<li><strong>将rpm包转换为cpio格式</strong></li>
</ul>
</li>
<li><p>cpio</p>
<ul>
<li>一个标准工具，用于创建软件档案文件和<strong>从档案文件中提取文件</strong></li>
<li>这个命令并不知道从哪里提取文件，所以一般使<strong>用管道符输入给cpio</strong>，或者用重定向，让cpio从这些文件中提取</li>
</ul>
</li>
</ul>
<p>cpio用重定向的使用格式为</p>
<ul>
<li>cpio -idv &lt; [文件|设备]<ul>
<li>-i copy-in 模式，提取</li>
<li>-d 提取时自动新建目录</li>
<li>-v 显示提取过程</li>
<li>利用<strong>管道符或者重定向得知从哪里提取数据</strong>！</li>
<li>-idv 是提取的标准选项！</li>
</ul>
</li>
</ul>
</blockquote>
<p>例如，系统命令ls是放在bin下的ls当中</p>
<pre><code>whereis ls
</code></pre><p>返回</p>
<pre><code>ls:/bin/ls
</code></pre><p>但是如果误操作，将/bin/ls文件删了，这时ls命令会消失。解决方法是再找一台linux，把这台linux上的/bin/ls拷贝过来只要版本一致就行。</p>
<p>如果没有另一台linux，则可以考虑通过rpm包提取文件的方法。包括linux系统在内的所有软件都是通过rpm包安装出来的，因此所有系统文件都在rpm包当中，可以从中提取出需要的系统文件。</p>
<p>第一步：了解到要提取的文件属于哪个包</p>
<pre><code>rpm -qf /bin/ls
</code></pre><p>第二步：提取文件,从光盘中的某个rpm包中提取</p>
<pre><code>rpm2cpio /mnt/cdrom/packages/coreutils-8.4-19.el6.i686.rpm |cpio -idv ./bin/ls
</code></pre><p>通过rpm2cpio命令，将/mnt/cdrom/packages/coreutils-8.4-19.el6.i686.rpm转换成cpio格式；通过管道符，<code>cpio -idv</code> 命令知道从前面的数据中提取文件，提取的文件就是./bin下的ls文件；<strong>. 表示</strong>把提取出的文件会保存在当前目录下。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux软件安装管理（一）：初识Linux软件包及其分类]]></title>
      <url>http://shengdeng.github.io/2016/06/11/Linux%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%88%9D%E8%AF%86Linux%E8%BD%AF%E4%BB%B6%E5%8C%85%E5%8F%8A%E5%85%B6%E5%88%86%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p><strong>内容提要</strong></p>
<ol>
<li>软件包发布的两种形式：源码包和RPM包</li>
<li>写脚本安装软件</li>
<li>利用yum软件管理RPM的依赖</li>
</ol>
<a id="more"></a>
<h1 id="1-软件包分类"><a href="#1-软件包分类" class="headerlink" title="1. 软件包分类"></a>1. 软件包分类</h1><p>Linux的软件的发布形式有如下两种</p>
<ul>
<li><p><strong>源代码包</strong>：需手工编译</p>
<ul>
<li>绝大多数开源软件都是直接<strong>以源代码形式发布</strong>，被打包成一个<strong>.tar.gz</strong>的归档压缩文件</li>
<li>源代码需要<strong>手工编译</strong>才能运行，所以针对特定运行环境的编译出来的软件兼容性和可控性好</li>
<li>优点：<ul>
<li>开源，学习者可以直接看到高手写的代码，可以修改源代码</li>
<li>编译安装，更加适合自己的系统，更稳定高效</li>
<li>卸载方便：直接删除安装文件目录就行</li>
</ul>
</li>
<li>缺点：<ul>
<li>安装过程步骤较多，尤其是大型软件</li>
<li>编译过程时间长，编译时开源软件需要大量依赖其他开源软件包</li>
<li>万一源码问题导致编译出错，不是自己写的很难解决</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>二进制包（即RPM包）</strong>：RPM命令管理</p>
<ul>
<li>Linux系统就是用RPM包安装的，系统光盘中软件的系统默认包</li>
<li><strong>厂商</strong>把源码包<strong>编译好</strong>以后，发布的二进制包</li>
<li>优点：<ul>
<li>软件包管理简单，通过命令就可以实现安装、升级、卸载</li>
<li>软件安装速度快</li>
</ul>
</li>
<li>缺点：<ul>
<li>不能看到源代码</li>
<li>依赖性：开源软件一般都会大量使用其他软件的功能，所以开源软件会有大量的依赖关系；安装中出现很多问题基本都是要解决依赖关系</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="3-写脚本安装大型复杂软件"><a href="#3-写脚本安装大型复杂软件" class="headerlink" title="3. 写脚本安装大型复杂软件"></a>3. 写脚本安装大型复杂软件</h1><p>脚本安装：</p>
<ul>
<li>有的软件安装，需要几十个软件包联合安装</li>
<li>高手把复杂的软件安装过程<strong>写成了shell脚本</strong>，安装直接执行脚本即可。但<strong>实际安装还是编译源码包或者RPM包</strong></li>
<li>优点：安装步骤少</li>
<li>缺点：丧失了自定义性，自定义时需要修改脚本</li>
</ul>
<h1 id="2-利用yum软件管理RPM依赖"><a href="#2-利用yum软件管理RPM依赖" class="headerlink" title="2. 利用yum软件管理RPM依赖"></a>2. 利用yum软件管理RPM依赖</h1><ul>
<li>是用来管理RPM的软件</li>
<li>用来解决RPM的依赖关系问题。特性：<ul>
<li><strong>自动解决依赖关系</strong></li>
<li>可以对RPM进行分组，并基于组进行安装操作：原本RPM每个软件包独立的；基于组的安装即一次性安装一类软件，只需要安装一个组，一类软件就安装好了</li>
<li>引入<strong>仓库概念</strong>，支持多个仓库</li>
<li>配置简单</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[linux服务管理（一）：服务分类]]></title>
      <url>http://shengdeng.github.io/2016/06/11/linux%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86%EF%BC%9A%E6%9C%8D%E5%8A%A1%E5%88%86%E7%B1%BB/</url>
      <content type="html"><![CDATA[<h1 id="1-Linux服务的分类"><a href="#1-Linux服务的分类" class="headerlink" title="1.Linux服务的分类"></a>1.Linux服务的分类</h1><p>在windous中，有些软件包安装以后成为应用，有些成为系统服务，Linux也一样，Linux服务分类</p>
<ul>
<li><strong>RPM包默认安装的服务</strong>：整个Linux系统都是用RPM软件包安装的<ul>
<li><strong>独立的服务</strong>：大多数服务都是独立的，直接在内存中，直接快速响应用户，但耗费内存</li>
<li><strong>基于xinetd服务</strong>：xinetd服务本身是独立的，就在内存中，唯一的功能是管理一系列服务；通过xinetd服务区访问那些被管理的服务；所有基于xinetd的服务不占内存</li>
</ul>
</li>
<li><strong>源码包安装的服务</strong></li>
</ul>
<h1 id="2-启动与自启动"><a href="#2-启动与自启动" class="headerlink" title="2.启动与自启动"></a>2.启动与自启动</h1>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[下载和编译Spark]]></title>
      <url>http://shengdeng.github.io/2016/06/11/%E7%BC%96%E8%AF%91Spark/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="1-下载Spark"><a href="#1-下载Spark" class="headerlink" title="1. 下载Spark"></a>1. 下载Spark</h1><p>登陆<a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">Spark官网的下载页面</a>可以下载Spark源码或预编译好的包，进入页面后下载有如下5步：</p>
<ol>
<li>选择Spark的版本；</li>
<li>选择包的类型：有源码包，也有预编译的包如Pre-built for CDH4，即运行在CDH4集群上的Spark；</li>
<li>选择下载的类型：镜像文件或者直接下载文件</li>
<li>下载链接</li>
<li>校验和下载</li>
</ol>
<p>还需注意，如官网的所述</p>
<blockquote>
<p>Spark 1.6.1 runs on <strong>Java 7+</strong>, Python 2.6+ and R 3.1+. For the Scala API, Spark 1.6.1 uses <strong>Scala 2.10</strong>. You will need to use a compatible Scala version (2.10.x).</p>
</blockquote>
<p>即Spark使用的是Scala2.10版本；scala2.11的用户需要下载Spark源码然后用Scala2.11进行编译，编译方法见<a href="http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211" target="_blank" rel="external">Building for scala 2.11</a>。</p>
<p>也就是说，Spark需要针对特定的运行环境进行编译，比如CDH或Hadoop的版本，Scala的版本等。实际生产环境使用的CDH版本常常没有对应的预编译包，不同的生产环境使用的Hadoop和Hive的版本也不一样，所以需要自行编译，官方只是针对某一部分版本进行了编译。<a href="http://spark.apache.org/docs/latest/building-spark.html" target="_blank" rel="external">官方指导文档</a>给出了编译Spark的方法。</p>
<p>此处下载Spark源码，接下来对Spark进行编译。</p>
<p>注意：下载后的源文件要修改成普通用户权限，否则会报权限的错误。</p>
<h2 id="下载页面其他信息"><a href="#下载页面其他信息" class="headerlink" title="下载页面其他信息"></a>下载页面其他信息</h2><p>在官方的下载页面，还有其他有用的信息</p>
<ul>
<li>Link with Spark 给出了Spark的当前版本在maven仓库中的三元组信息</li>
<li>Spark Source Code Management 是关于从github上clone Spark版本分支的方法，也即从github获得源码的方法<ul>
<li>Spark是apache的项目托管在github上：<a href="https://github.com/apache/spark" target="_blank" rel="external">https://github.com/apache/spark</a></li>
<li>选择分支branch可以看到最新的分支是Spark 2.0</li>
</ul>
</li>
</ul>
<h1 id="2-Spark1-x的三种编译方式"><a href="#2-Spark1-x的三种编译方式" class="headerlink" title="2. Spark1.x的三种编译方式"></a>2. Spark1.x的三种编译方式</h1><ul>
<li>SBT编译<ul>
<li>Simple Build Tool</li>
<li>专门针对Scala的工程项目构建 </li>
</ul>
</li>
<li>Maven编译<ul>
<li>项目管理构建工具</li>
<li>Spark是scala语言写的，但是支持maven编译</li>
<li>一定要提前安装maven；一定要保证联网</li>
<li>spark源文件中有pom.xml，就是用来maven编译的</li>
</ul>
</li>
<li>打包编译make-distribution.sh<ul>
<li>实际上就是maven编译，只不过写成.sh的脚本运行</li>
<li>本文主要介绍这种编译方式</li>
<li>spark源文件中有make-distribution.sh</li>
</ul>
</li>
</ul>
<h1 id="3-编译官方指导"><a href="#3-编译官方指导" class="headerlink" title="3. 编译官方指导"></a>3. 编译官方指导</h1><h2 id="2-1-编译环境的准备"><a href="#2-1-编译环境的准备" class="headerlink" title="2.1 编译环境的准备"></a>2.1 编译环境的准备</h2><ol>
<li>Linux上安装好JDK</li>
<li>Linux上安装好maven</li>
<li>Linux上安装Scala 2.10<br>注意<a href="http://spark.apache.org/docs/latest/building-spark.html" target="_blank" rel="external">官方指导</a>中建议的JDK和maven的版本<blockquote>
<p>Building Spark using Maven requires <strong>Maven 3.3.3</strong> or newer and <strong>Java 7+</strong>. The Spark build can supply a suitable Maven binary; see below.</p>
</blockquote>
</li>
</ol>
<p>scala推荐2.10版本，2.11版本需要自行编译Spark参见官方指导的<strong>Building for Scala 2.11</strong></p>
<pre><code>./dev/change-scala-version.sh 2.11
mvn -Pyarn -Phadoop-2.4 -Dscala-2.11 -DskipTests clean package
</code></pre><h2 id="2-2-maven编译"><a href="#2-2-maven编译" class="headerlink" title="2.2 maven编译"></a>2.2 maven编译</h2><p>参见官方指导的<strong>building with build/mvn</strong></p>
<pre><code>build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package
</code></pre><ul>
<li>-Pyarn表示支持yarn</li>
<li>-Phadoop-2.4 -Dhadoop.version=2.4.0表示hadoop的版本</li>
<li>-DskipTests 编译的时候跳过测试</li>
<li>clean package 打包</li>
</ul>
<p>注意在编译的时候，可能会出现<strong>out of memory</strong>的错误，避免这个错误，需要进行设置，参考<strong>Setting up maven’s memory usage</strong> 中说明的参数MAVEN_OPTS。</p>
<pre><code>export MAVEN_OPTS=&quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&quot;
</code></pre><p>即设置maven编译时最多使用的内存堆栈的大小。使用打包编译，就不用设置。</p>
<p>在指定hadoop版本的时候可以参考<strong>Specifying the Hadoop Version</strong>中的说明，也给了很多实例，例如针对CDH 4.2的编译方法：</p>
<pre><code>mvn -Dhadoop.version=2.0.0-mr1-cdh4.2.0 -Phadoop-1 -DskipTests clean package
</code></pre><p>是否支持Hive和JDBC的编译方法，参考<strong>Building With Hive and JDBC Support</strong>，例如</p>
<pre><code># Apache Hadoop 2.4.X with Hive 13 support
mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver -DskipTests clean package
</code></pre><h2 id="2-3-利用make-distribution-sh编译"><a href="#2-3-利用make-distribution-sh编译" class="headerlink" title="2.3 利用make-distribution.sh编译"></a>2.3 利用make-distribution.sh编译</h2><p>打开make-distribution.sh文件查看内容</p>
<ul>
<li>177行设置了参数MAVEN_OPTS避免内存溢出</li>
<li>182行编译的命令，就是mvn命令</li>
<li><p>54行开始，是解析参数，这些参数hadoop、yarn、hive过时了，需要设置–name–tgz等参数</p>
<p>  ./make-distribution.sh –name custom-spark –tgz -Psparkr -Phadoop-2.4 -Phive -Phive-thriftserver -Pyarn</p>
</li>
</ul>
<p>在实际生产中，用的多的是CDH的hadoop版本，可以在 <a href="http://archive.cloudera.com/cdh5/" target="_blank" rel="external">http://archive.cloudera.com/cdh5/</a> 中下载和查到CDH版本，因此可以写成</p>
<pre><code># cdh和hive都指定版本
./make-distribution.sh --name custom-spark --tgz -Pyarn -Psparkr -Phadoop-2.4 -Dhadoop.version=2.6.0-CDH5.4.0 -Phive-0.13.1 -Phive-thriftserver 
</code></pre><p>注意：maven编译一定要翻墙，因为需要下载很多依赖包。出现几次错误是正常的。make-distribution.sh运行以后，半天才有反应很迟钝，这是因为make-distribution.sh会去进行很多操作，为了节省时间你可以进行一些修改129-138行注释掉（每一行都注释），这几行是解析spark、hadoop、hive的版本，我们输入命令时加上版本信息就行（草稿）</p>
<h1 id="4-编译spark实战"><a href="#4-编译spark实战" class="headerlink" title="4.编译spark实战"></a>4.编译spark实战</h1>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[前沿追踪：热门大数据技术TOP10]]></title>
      <url>http://shengdeng.github.io/2016/06/10/%E5%89%8D%E6%B2%BF%E8%BF%BD%E8%B8%AA%EF%BC%9A%E7%83%AD%E9%97%A8%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AFTOP10/</url>
      <content type="html"><![CDATA[<p>随着大数据分析市场的快速扩展到各行业务，哪些大数据技术是刚需？哪些技术有极大的潜在价值？弗雷斯特研究公司发布的<a href="http://www.forbes.com/sites/gilpress/2016/03/14/top-10-hot-big-data-technologies/#5b01c3da7f26" target="_blank" rel="external">TechRadar: Big Data, Q1 2016</a>报告评估了数据生命周期中22项大数据技术的成熟度和发展状态。本文介绍了排名靠前的10项大数据技术，可为大数据从业者的职业发展方向提供参考。</p>
<a id="more"></a>
<p>报告从四个维度（技术更新时间、发展阶段、商业价值、技术成功度）对这22项技术进行评估：</p>
<ul>
<li>这22项技术分别处在5个发展阶段：<ul>
<li>creation初创期、survival快速发展期、growth平稳成长期、equilibrium停滞开发期、decline衰落期</li>
<li>5个发展阶段的版本更替时间基本上呈现逐渐降低的规律，creation期版本更新最快，equilibrium期技术已经成熟，因而版本更新最慢。</li>
</ul>
</li>
<li>从商业价值角度来看，22项技术处在4个层次<ul>
<li>negative、low、medium、high</li>
<li>越成熟的技术，技术更新时间越慢、商业价值越高</li>
</ul>
</li>
<li>从技术的成功与否来看，22项技术被划分成三个水平：<ul>
<li>significant success非常成功有重大突破、moderate success基本成功、minimal success效果微乎其微<br><img src="http://7xtorv.com1.z0.glb.clouddn.com/ForresterBigData_2.jpg" alt=""></li>
</ul>
</li>
</ul>
<p>根据上图的研究结果最热的十个大数据技术：</p>
<ol>
<li><strong>预测分析Predictive analytics:</strong> 随着现在硬件和软件解决方案的成熟，许多公司利用大数据技术来收集海量数据、训练模型、优化模型，并发布预测模型来<strong>提高业务水平</strong>或者<strong>避免风险</strong>；</li>
<li><strong>NoSQL数据库NoSQL databases:</strong> 非关系型数据库包括Key-value型（Redis）数据库、文档型（MonogoDB）数据库、图型（Neo4j）数据库；</li>
<li><strong>搜索和知识发现Search and knowledge discovery:</strong> 支持信息的自动抽取，可以从多数据源<strong>洞察结构化数据和非结构化数据</strong>，各种源包括文件系统, 数据库, 流, APIs和任何平台、应用app.</li>
<li><strong>流式分析Stream analytics:</strong> 软件可以对多个高吞吐量的数据源进行<strong>实时的</strong>清洗、聚合和分析；</li>
<li><strong>内存数据结构In-memory data fabric:</strong> 通过<strong>动态随机内存访问（dynamic random access memory，DRAM）、Flash和SSD</strong>等分布式存储系统提供海量数据的低延时访问和处理；</li>
<li><strong>分布式存储系统Distributed file stores:</strong> 分布式存储是指存储节点大于一个、数据保存多副本以及高性能的计算网络；</li>
<li><strong>数据可视化Data virtualization:</strong> 数据可视化技术是指对各类型数据源（包括Hadoop上的海量数据以及实时和接近实时的分布式数据）进行显示；</li>
<li><strong>数据整合Data integration:</strong> 通过亚马逊弹性MR（EMR）、Hive、Pig、Spark、MapReduce、Couchbase、Hadoop和MongoDB等软件进行业务数据整合；</li>
<li><strong>数据预处理Data preparation:</strong> 对数据源进行清洗、裁剪，并共享多样化数据来加快数据分析；</li>
<li><strong>数据校验Data quality:</strong>对分布式存储系统和数据库上的海量、高频率数据集进行数据校验，去除非法数据，补全缺失。</li>
</ol>
<p>以上10项技术均处在significant success非常成功的轨迹线上，而且都处于技术发展中的成熟阶段，前8项技术处在growth平稳成长期，后两项技术处在survival快速发展期。前两项技术被评估为具有较高的商业价值，第三四项被评估为中等商业价值，剩下的6项技术被评估为较低的商业价值，不用怀疑，因为后6项新兴技术还在快速发展且并不十分成熟可靠。</p>
<p>Why did I add to the list of hottest technologies two that are still in the Survival phase—data preparation and data quality? In the same report, Forrester also provides the following data from its Q4 2015 survey of 63 big data vendors:</p>
<p>What is the level of customer interest in each of the following capabilities? (% answering “very high”)</p>
<blockquote>
<p>Data preparation and discovery 52%</p>
<p>Data integration 48%</p>
<p>Advanced analytics 46%</p>
<p>Customer analytics 46%</p>
<p>Data security 38%</p>
<p>In-memory computing 37%</p>
</blockquote>
<p>While Forrester predicts that a few standalone vendors of data preparation will survive, it believes this is “an essential capability for achieving democratization of data,” or rather, its analysis, letting data scientists spend more time on modeling and discovering insights and allowing more business users to have fun with data mining.  Data Quality includes data security from the table above, in addition to other features ensuring decisions are based on reliable and accurate data. Forrester “expects that data quality will have significant success in the coming years as firms formalize a data certification process. Data certification efforts seek to guarantee that data meets expected standards for quality; security; and regulatory compliance supporting business decision-making, business performance, and business processes.”</p>
<p>“Big Data” as a topic of conversation has reached mainstream audiences probably far more than any other technology buzzword before it. That did not help the discussion of this amorphous term, defined for the masses as “the planet’s nervous system” (see my rant here) or as “Hadoop” for technical audiences.  Forrester’s report helps clarify the term, defining big data as the ecosystem of 22 technologies, each with its specific benefits for enterprises and, through them, consumers.</p>
<p>Big data, specifically one its attributes, big volume, has recently gave rise to a new general topic of discussion, Artificial Intelligence. The availability of very large data sets is one of the reasons Deep Learning, a sub-set of AI, has been in the limelight, from identifying Internet cats to beating a Go champion.  In its turn, AI may lead to the emergence of new tools for collecting and analyzing data.</p>
<p>Says Forrester: “In addition to more data and more computing power, we now have expanded analytic techniques like deep learning and semantic services for context that make artificial intelligence an ideal tool to solve a wider array of business problems. As a result, Forrester is seeing a number of new companies offering tools and services that attempt to support applications and processes with machines that mimic some aspects of human intelligence.”</p>
<p>Prediction is difficult, especially about the future, but it’s a (relatively) safe bet that the race to mimic elements of human intelligence, led by Google, Facebook, Baidu, Amazon, IBM, and Microsoft, all with very deep pockets, will change what we mean by “big data” in the very near future.</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kafka1.0生态系统]]></title>
      <url>http://shengdeng.github.io/2016/06/02/Kafka%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E7%B3%BB%E7%BB%9F/</url>
      <content type="html"><![CDATA[<p>本文给出列出了<a href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem" target="_blank" rel="external">kafka官方wiki文档</a>中声称已经集成了kafka的各种工具，组成了kafka的生态系统。但是这些工具集成kafka效果如何本文未作评估。</p>
<a id="more"></a>
<hr>
<h1 id="0-Kafka-Streams"><a href="#0-Kafka-Streams" class="headerlink" title="0.Kafka Streams"></a>0.Kafka Streams</h1><p><em>更新：2016年6月2日</em></p>
<p>Kafka Streams在几个月前由<a href="http://www.confluent.io/" target="_blank" rel="external">Confluent Platform</a>首先在其平台的技术预览中行提出，目前已经<strong>在Apache Kafka 0.10.0.0上可用</strong>了。</p>
<p>Kafka Streams其实是一套类库，它使得Apache Kafka可以<strong>拥有流处理的能力</strong>。Kafka Streams包含了一整套描述常见流操作的高级语言API（比如 joining, filtering以及aggregating records），这使得开发者可以快速开发强大的流处理应用程序。Kafka Streams提供了状态和无状态的处理能力，并且可以部署在很多系统之上： Kafka Streams应用程序可以运行在YARN、Mesos、Docker containers上，甚至直接嵌入到现有的Java应用程序中。</p>
<hr>
<p><em>官网最后更新：2016年4月</em></p>
<h1 id="1-数据导入导出工具-Kafka-Connect"><a href="#1-数据导入导出工具-Kafka-Connect" class="headerlink" title="1.数据导入导出工具(Kafka Connect)"></a>1.数据导入导出工具(Kafka Connect)</h1><p>Kafka Connect是Kafka 0.9+的一个自带工具，通过connectors可以将大数据从其它系统导入到Kafka中，也可以从Kafka中导出到其它系统。 详情可以参考<a href="http://docs.confluent.io/2.0.0/connect/index.html" target="_blank" rel="external">Kafka Connect的官方文档</a>和我的文章。</p>
<h1 id="2-商业平台的kafka集成包-Distributions-amp-Packaging"><a href="#2-商业平台的kafka集成包-Distributions-amp-Packaging" class="headerlink" title="2.商业平台的kafka集成包(Distributions &amp; Packaging)"></a>2.商业平台的kafka集成包(Distributions &amp; Packaging)</h1><ul>
<li><strong>Confluent Platform</strong> 是首个以kafka为基础的流式数据平台- <a href="http://confluent.io/product/" target="_blank" rel="external">http://confluent.io/product/</a>. Downloads - <a href="http://confluent.io/downloads/" target="_blank" rel="external">http://confluent.io/downloads/</a>.</li>
<li><strong>Cloudera Kafka source</strong> <a href="https://github.com/cloudera-labs/kafka/tree/cdh5-0.8.2_1.1.0" target="_blank" rel="external">https://github.com/cloudera-labs/kafka/tree/cdh5-0.8.2_1.1.0</a> and release <a href="http://www.cloudera.com/content/cloudera/en/developers/home/cloudera-labs/apache-kafka.html" target="_blank" rel="external">http://www.cloudera.com/content/cloudera/en/developers/home/cloudera-labs/apache-kafka.html</a></li>
<li><strong>Hortonworks Kafka source and release</strong> <a href="http://hortonworks.com/hadoop/kafka/" target="_blank" rel="external">http://hortonworks.com/hadoop/kafka/</a><br>Stratio Kafka source for ubuntu <a href="http://repository.stratio.com/sds/1.1/ubuntu/13.10/binary/" target="_blank" rel="external">http://repository.stratio.com/sds/1.1/ubuntu/13.10/binary/</a> and for RHEL <a href="http://repository.stratio.com/sds/1.1/RHEL/" target="_blank" rel="external">http://repository.stratio.com/sds/1.1/RHEL/</a></li>
<li><strong>IBM Message Hub</strong> - <a href="https://developer.ibm.com/messaging/message-hub/" target="_blank" rel="external">https://developer.ibm.com/messaging/message-hub/</a> - Kafka as-a-service in a IBM’s Bluemix PaaS</li>
</ul>
<h1 id="3-与流处理框架集成-Stream-Processing"><a href="#3-与流处理框架集成-Stream-Processing" class="headerlink" title="3.与流处理框架集成(Stream Processing)"></a>3.与流处理框架集成(Stream Processing)</h1><ul>
<li><strong>Kafka Streams</strong> Built-in library as part of the Kafka project.</li>
<li><strong>Storm</strong> A stream-processing framework.</li>
<li><strong>Samza</strong> A YARN-based stream processing framework.</li>
<li><strong>Storm Spout</strong> Consume messages from Kafka and emit as Storm tuples</li>
<li><strong>Kafka-Storm</strong> Kafka 0.8, Storm 0.9, Avro integration</li>
<li><strong>SparkStreaming</strong> Kafka receiver supports Kafka 0.8 and above</li>
<li><strong>Flink</strong> Apache Flink has an integration with Kafka</li>
<li><strong>IBM Streams</strong> A stream processing framework with Kafka source and sink to consume and produce Kafka messages </li>
</ul>
<h1 id="4-与Hadoop集成-Integration"><a href="#4-与Hadoop集成-Integration" class="headerlink" title="4.与Hadoop集成(Integration)"></a>4.与Hadoop集成(Integration)</h1><ul>
<li><strong>Kafka Connect sink</strong> A sink for Kafka’s connector framework.</li>
<li><strong>Camus</strong> LinkedIn’s Kafka=&gt;HDFS pipeline. This one is used for all data at LinkedIn, and works great.</li>
<li><strong>Kafka Hadoop Loader</strong> A different take on Hadoop loading functionality from what is included in the main distribution.</li>
<li><strong>Flume</strong> Contains Kafka Source (consumer) and Sink (producer)</li>
<li><strong>KaBoom</strong> A high-performance HDFS data loader</li>
</ul>
<h1 id="5-与搜索查询系统集成-Search-and-Query"><a href="#5-与搜索查询系统集成-Search-and-Query" class="headerlink" title="5.与搜索查询系统集成(Search and Query)"></a>5.与搜索查询系统集成(Search and Query)</h1><ul>
<li><strong>ElasticSearch</strong> - This project, Kafka Standalone Consumer will read the messages from Kafka, processes and index them in ElasticSearch. There are also several Kafka Connect connectors for ElasticSeach.<br><strong>Presto</strong> The Presto Kafka connector allows you to query Kafka in SQL using Presto.<br><strong>Hive</strong> Hive SerDe that allows querying Kafka (Avro only for now) using Hive SQL</li>
</ul>
<h1 id="6-kafka的监控管理-Management-Consoles"><a href="#6-kafka的监控管理-Management-Consoles" class="headerlink" title="6.kafka的监控管理(Management Consoles)"></a>6.kafka的监控管理(Management Consoles)</h1><ul>
<li><strong>Kafka Manager</strong> - A tool for managing Apache Kafka.</li>
<li><strong>kafkat</strong> - Simplified command-line administration for Kafka brokers.</li>
<li><strong>Kafka Web Console</strong>- Displays information about your Kafka cluster including which nodes are up and what topics they host data for.</li>
<li><strong>Kafka Offset Monitor</strong> - Displays the state of all consumers and how far behind the head of the stream they are.</li>
<li><strong>Capillary</strong> – Displays the state and deltas of Kafka-based Apache Storm topologies. Supports Kafka &gt;= 0.8. It also provides an API for fetching this information for monitoring purposes.</li>
</ul>
<h1 id="7-AWS-Integratio"><a href="#7-AWS-Integratio" class="headerlink" title="7.AWS Integratio"></a>7.AWS Integratio</h1><ul>
<li>Automated AWS deployment</li>
<li>Kafka -&gt; S3 Mirroring tool from Pinterest.</li>
<li>Alternative Kafka-&gt;S3 Mirroring tool</li>
</ul>
<h1 id="8-Logging"><a href="#8-Logging" class="headerlink" title="8.Logging"></a>8.Logging</h1><ul>
<li>syslog (1M)</li>
<li>syslog producer : A producer that supports both raw data and protobuf with meta data for - deep analytics usage. </li>
<li>syslog-ng (<a href="https://syslog-ng.org/" target="_blank" rel="external">https://syslog-ng.org/</a>) is one of the most widely used open source log  collection tools, capable of filtering, classifying, parsing log data and forwarding it to a wide variety of destinations. Kafka is a first-class destination in the syslog-ng tool; details on the integration can be found at <a href="https://czanik.blogs.balabit.com/2015/11/kafka-and-syslog-ng/" target="_blank" rel="external">https://czanik.blogs.balabit.com/2015/11/kafka-and-syslog-ng/</a> .</li>
<li>klogd A python syslog publisher</li>
<li>klogd2 A java syslog publisher</li>
<li>Tail2Kafka A simple log tailing utility</li>
<li>Fluentd plugin Integration with Fluentd</li>
<li>Remote log viewer</li>
<li>LogStash integration Integration with LogStash and Fluentd</li>
<li>Syslog Collector written in Go</li>
<li>Klogger A simple proxy service for Kafka.</li>
<li>fuse-kafka: A file system logging agent based on Kafka</li>
<li>omkafka: Another syslog integration, this one in C and uses librdkafka library</li>
<li><p>logkafka Collect logs and send lines to Apache Kafka</p>
</li>
<li><p>Flume - Kafka plugins</p>
<ul>
<li>Flume Kafka Plugin - Integration with Flume</li>
<li>Kafka as a sink and source in Flume - Integration with Flume</li>
</ul>
</li>
</ul>
<h1 id="9-Metrics"><a href="#9-Metrics" class="headerlink" title="9.Metrics"></a>9.Metrics</h1><ul>
<li>Mozilla Metrics Service - A Kafka and Protocol Buffers based metrics and logging system</li>
<li>Ganglia Integration</li>
<li>SPM for Kafka</li>
<li>Coda Hale Metric Reporter to Kafka</li>
</ul>
<h1 id="10-打包和部署-Packing-and-Deployment"><a href="#10-打包和部署-Packing-and-Deployment" class="headerlink" title="10.打包和部署(Packing and Deployment)"></a>10.打包和部署(Packing and Deployment)</h1><ul>
<li>RPM packaging</li>
<li>Debian packaging<a href="https://github.com/tomdz/kafka-deb-packaging" target="_blank" rel="external">https://github.com/tomdz/kafka-deb-packaging</a></li>
<li>Puppet Integration<ul>
<li><a href="https://github.com/miguno/puppet-kafka" target="_blank" rel="external">https://github.com/miguno/puppet-kafka</a></li>
<li><a href="https://github.com/whisklabs/puppet-kafka" target="_blank" rel="external">https://github.com/whisklabs/puppet-kafka</a></li>
</ul>
</li>
<li>Dropwizard packaging</li>
</ul>
<h1 id="11-Kafka-Camel-Integration"><a href="#11-Kafka-Camel-Integration" class="headerlink" title="11.Kafka Camel Integration"></a>11.Kafka Camel Integration</h1><ul>
<li><a href="https://github.com/ipolyzos/camel-kafka" target="_blank" rel="external">https://github.com/ipolyzos/camel-kafka</a></li>
<li><a href="https://github.com/BreizhBeans/camel-kafka" target="_blank" rel="external">https://github.com/BreizhBeans/camel-kafka</a></li>
</ul>
<h1 id="12-其他杂项-Misc"><a href="#12-其他杂项-Misc" class="headerlink" title="12.其他杂项(Misc.)"></a>12.其他杂项(Misc.)</h1><ul>
<li>Kafka Websocket - A proxy that interoperates with websockets for delivering Kafka data to browsers.</li>
<li>KafkaCat - A native, command line producer and consumer.</li>
<li>Kafka Mirror - An alternative to the built-in mirroring tool</li>
<li>Ruby Demo App </li>
<li>Apache Camel Integration</li>
<li>Infobright integration</li>
<li>Riemann Consumer of Metrics</li>
<li>stormkafkamom – curses-based tool which displays state of Apache Storm based Kafka consumers (Kafka 0.7 only).</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[初识Kafka：分布式消息系统比较]]></title>
      <url>http://shengdeng.github.io/2016/06/02/%E5%88%9D%E8%AF%86Kafka/</url>
      <content type="html"><![CDATA[<p><strong>主要提要</strong></p>
<ol>
<li>消息队列系统的应用场景</li>
<li>流行的消息队列系统ActiveMQ、RabbitMQ、Kafka的比较</li>
<li>kafka支持的客户端语言<a id="more"></a>
<h1 id="1-消息系统的使用场景"><a href="#1-消息系统的使用场景" class="headerlink" title="1.消息系统的使用场景"></a>1.消息系统的使用场景</h1>在我们大量使用<strong>分布式数据库</strong>、<strong>分布式计算集群</strong>的时候，会遇到这样的问题：</li>
</ol>
<ul>
<li>分析用户行为(pageviews)，以便设计出更好的广告位</li>
<li>对用户搜索关键词进行统计，分析出当前的流行趋势</li>
<li>有些数据，存数据库浪费，直接存硬盘操作效率低</li>
</ul>
<p>这些场景具有共同的特征：</p>
<ul>
<li>数据是由上一个模块产生</li>
<li>下一个模块对上一个模块的数据进行计算、处理、统计和分析</li>
</ul>
<p>以上场景<strong>适合使用消息系统</strong>，尤其是分布式消息系统</p>
<h1 id="2-Kafka定义"><a href="#2-Kafka定义" class="headerlink" title="2.Kafka定义"></a>2.Kafka定义</h1><p>关键点：</p>
<ul>
<li>是一个分布式消息系统</li>
<li>由LinkedIn使用Scala编写，用作LinkedIn的<ul>
<li>活动流(Activity Stream)的基础</li>
<li>运营数据处理管道(Pipeline)的基础</li>
</ul>
</li>
<li>优势特点<ul>
<li>高水平扩展</li>
<li>高吞吐量</li>
</ul>
</li>
</ul>
<p>应用领域：已被多加不同类型的公司作为<strong>多种类型的数据管道</strong>和<strong>消息系统</strong>使用</p>
<p>因为kafka的高水平扩展和高吞吐的能力，目前越来越多的<strong>开源分布式处理系统</strong>，如flume(用于日志收集)、Storm(用于实时数据处理)、Spark(用于内存数据处理)、elasticsearch(用于全文检索)都支持<strong>与Kafka集成</strong>。</p>
<h1 id="3-目前开源分布式消息系统比较"><a href="#3-目前开源分布式消息系统比较" class="headerlink" title="3.目前开源分布式消息系统比较"></a>3.目前开源分布式消息系统比较</h1><table>
<thead>
<tr>
<th>比较项</th>
<th>ActiveMQ</th>
<th>RabbitMQ</th>
<th>Kafka</th>
</tr>
</thead>
<tbody>
<tr>
<td>公式/社区</td>
<td>Apache</td>
<td>Mozilla Public License</td>
<td>Apache/LinkedIn</td>
</tr>
<tr>
<td>开发语言</td>
<td>java</td>
<td>Erlang</td>
<td>Scala</td>
</tr>
<tr>
<td><strong>支持的协议</strong></td>
<td>OpenWire\STOMP\REST\XMPP\ <strong>AMQP</strong></td>
<td><strong>AMQP</strong></td>
<td>类似AMQP</td>
</tr>
<tr>
<td><strong>事务</strong></td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>集群</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>负载均衡</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td><strong>动态扩容</strong></td>
<td>不支持</td>
<td>不支持</td>
<td>支持(Zookeeper)</td>
</tr>
</tbody>
</table>
<p>各种分布式消息队列MQ对比详解：</p>
<ul>
<li><strong>开发语言</strong>：JAVA和Scalca是运行在JVM上的两种语言；Erlang和最近比较火的Go一样，在语言级别就支持高并发的服务器端开发语言，所以RabbitMQ天生就有很高的性能</li>
<li><strong>支持的协议</strong>：消息系统都是消息中间件，所以设计和实现消息系统都依靠一套相关的协议，其中ActiveMQ支持众多的协议；RabbitMQ严格按照AMQP协议进行设计实现，受到了很多约束，所以设计上不如Kafka精妙；Kafka仿照AMQP设计了一套面向高性能但是并不通用的协议;AMQP(Advanced Message Queuing Protocol)高级消息队列协议</li>
<li><strong>事务</strong>：事务的概念最早出现在传统的数据库中，多个操作提交，这些操作要么全部成功要么全部失败，不可能部分成功部分失败，例如在转账时，<strong>付款和收款必须是同一个事物</strong>。事务对应到分布式消息队列中，就是多条消息一起发送，要么全部发送成功要么全部失败，不可能部分成功部分失败。目前只有ActiveMQ支持事务，RabbitMQ、Kafka以追求更高的性能为目标，所以不支持事务</li>
<li><strong>集群</strong>：多台服务器组成，消息系统组成的集群增加或者减少一台服务器对生产者和消费者是透明的，无感知的</li>
<li><strong>负载均衡</strong>：大量生产者和消费者向消息系统发出请求，消息系统必须能够均衡这些请求，使得每一台服务器的请求数达到平衡，避免某台服务器超负荷</li>
<li><strong>动态扩容</strong>：若不支持动态扩容，则要先停止服务、再停掉消息系统，增加减少服务器，重启消息系统，最后重启服务，这对很多公司是不可接受的。只有Kafka支持，动态增加减少服务器并不影响生产环境的服务。</li>
</ul>
<p>此外，</p>
<ul>
<li>ActiveMQ：还支持Java消息服务（Java Message Service，JMS）的消息中间件，意味着客户端可以使用java编写的程序，可以和消息系统无缝的进行通信</li>
<li>由于Kafka的高吞吐量和高水平扩展的能力，阿里巴巴的metaq、rocketmq等各种消息系统要么改造自Kafka要么借鉴kafka的思想</li>
<li>ActiveMQ、RabbitMQ和Kafka相比不具有高吞吐量和高水平扩展的能力</li>
<li>kafka动态扩容是通过zookeeper来实现的，kafka增加或者减少节点，都会在zookeeper上触发相应的事件，kafka系统会捕获这些事件来进行新一轮的负载均衡；客户端也可以捕获这些事件。</li>
</ul>
<h1 id="4-AMQP协议"><a href="#4-AMQP协议" class="headerlink" title="4.AMQP协议"></a>4.AMQP协议</h1><p>三种比较流行的消息队列要么支持AMQP协议，要么借鉴了AMQP协议思想。AMQP协议主要由一下三个组件组成</p>
<p>【<strong>producer</strong>】→(push)→【<strong>broker</strong>】←(pull)←【<strong>comsumer</strong>】</p>
<ul>
<li>消费者：从消息队列中请求消息的客户端应用程序</li>
<li>生产者：向broker发送消息的客户端应用程序</li>
<li>AMQP服务器端(broker):用来接收生产者发送的消息并将这些消息路由给服务器中的队列；例如，kafka将生产者发送的消息动态的添加到磁盘，并给每一条消息一个偏移量</li>
</ul>
<h1 id="5-kafka支持的客户端语言"><a href="#5-kafka支持的客户端语言" class="headerlink" title="5.kafka支持的客户端语言"></a>5.kafka支持的客户端语言</h1><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-For0.8.x" target="_blank" rel="external">官方wiki中指出</a>，kafka客户端支持当前大部分主流语言，包括</p>
<ul>
<li>Producer Daemon</li>
<li>Python</li>
<li>Go (AKA golang)</li>
<li>C/C++</li>
<li>.net</li>
<li>Clojure</li>
<li>Ruby</li>
<li>Node.js</li>
<li>Alternative Java Clients</li>
<li>Storm </li>
<li>Scala DSL </li>
<li>HTTP REST </li>
<li>JRuby </li>
<li>Perl</li>
<li>stdin/stdout</li>
<li>Erlang</li>
<li>PHP</li>
<li>Rust</li>
</ul>
<p>可以使用以上任何一种语言和kafka服务器进行通信，即编写自己的consumer程序和produce程序。一个分布式系统支持的客户端语言是很重要的，因为不同的开发人员使用不同的语言是很常见的现象，若只支持一种语言则会提高开发和维护的成本。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[在VMware上安装Liunx CentOs系统]]></title>
      <url>http://shengdeng.github.io/2016/05/25/%E5%9C%A8VMware%E4%B8%8A%E5%AE%89%E8%A3%85Liunx%E7%B3%BB%E7%BB%9F/</url>
      <content type="html"><![CDATA[<h1 id="1-Linux系统下载"><a href="#1-Linux系统下载" class="headerlink" title="1. Linux系统下载"></a>1. Linux系统下载</h1><p><a href="https://www.centos.org/download/" target="_blank" rel="external">CentOs官网</a>下载，免费</p>
<h1 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h1><h2 id="2-1-硬件设置"><a href="#2-1-硬件设置" class="headerlink" title="2.1 硬件设置"></a>2.1 硬件设置</h2><ul>
<li>内存至少628Mb,少于828Mb时linux只能使用字符界面安装，不再使用图形界面；</li>
<li>CD/DVD光驱选择使用ISO镜像，选在下载好的ISO包</li>
<li>网卡设置</li>
<li>打开电源：点击打开电源，相当于开机，才会有界面</li>
<li>挂起 功能：把虚拟机当前状态卡住；只要点恢复就可以恢复过来。</li>
</ul>
<h2 id="2-2-界面安装"><a href="#2-2-界面安装" class="headerlink" title="2.2 界面安装"></a>2.2 界面安装</h2><ol>
<li>迅速的按下F2，进去BIOS界面。计算机默认通过硬盘启动，需要改成通过<strong>光盘启动</strong>。<ul>
<li>主板的操作系统就是BIOS系统，用于计算机基本信息设置，比如主板信息、系统时间</li>
<li>选择Boot菜单，将CD-ROM Drive移动到第一个</li>
<li>VMware中不需要此步操作，会智能的选择光盘启动。</li>
<li>真实机在安装时需要调整光盘启动，安装完后调整回硬盘启动。否则每次都从光盘重新安装。</li>
</ul>
</li>
<li><strong>光盘启动界面</strong>有五个选项：<ul>
<li>安装或者升级现有操作系统（选这个！）</li>
<li>安装操作系统，且使用基本显卡驱动</li>
<li>修复已安装的系统：进入系统修复模式</li>
<li>退出安装，并从硬盘启动（已安装了系统，又从光盘进去了）</li>
<li>内存检测</li>
</ul>
</li>
<li>选择第一个后，Linux开始进行<strong>自检</strong>，内存大于628Mb,才能进入图形安装界面（更简单可靠），否则字符安装</li>
<li>弹出Disc Found 询问是否需要检测光盘。下载好的光盘无需检测，选skip</li>
<li>自动检测显卡和内存，只要支持图形安装的要求，就可以进入图形安装欢迎界面<ul>
<li>ctrl+alt+enter，全屏显示</li>
</ul>
</li>
<li>安装语言：选简体中文（如果选英文，那么后面的安装包一定要选择中文字体，否则要手工安装中文字体）</li>
<li>选键盘：美式键盘</li>
<li>选择基本存储；提示你，可能会把硬盘中数据清空，选择是</li>
<li>起一个主机名（先用默认的，Linux对主机名并不敏感；在局域网内，windows系统IP地址不能重，主机名也不能重，否则两台相同主机名的计算机不能通信）</li>
<li>选择时区：默认上海</li>
<li>root用户设置密码</li>
<li><p>询问<strong>如何分区、挂载    </strong></p>
<ul>
<li>选择：创建自定义布局<ul>
<li>有一块sda(/dev/sda)硬盘    </li>
</ul>
</li>
<li>创建→标准分区<ul>
<li>RAID分区、LVM是高级文件系统的分区</li>
</ul>
</li>
<li>挂载点：<strong>/boot</strong>（任何目录都可以，建议先分/boot分区，然后剩余空间都给/分区）→文件系统类型：ext4→大小：200Mb<ul>
<li>/boot分区会默认被分为sda1,因为/boot用于系统启动，所以他一定会放在硬盘最开始的部分，如果强制改成sda2，即不在硬盘开始部分，可能会造成系统无法启动</li>
<li>sda1、sda2这些分区号是系统自动分配的，看懂就行</li>
</ul>
</li>
<li>创建→标准分区→挂载点：不选→文件系统类型：<strong>swap</strong>→大小：1Gb<ul>
<li>swap分区在文件系统中选；没有挂载点目录，这部分空间，不是给用户用的，是给系统内核直接调用的</li>
<li>swap其实对系统性能没多大影响，1Gb差不多了</li>
</ul>
</li>
<li>创建→标准分区→挂载点：<strong>/home</strong>→ext4→2Gb<ul>
<li>服务器如果用作文件服务器，允许用户上传下载数据到/home中，单独分区，可以保证数据的安全性</li>
<li>其他的目录分区也<strong>不是必须的</strong>，根据使用用途自定义</li>
</ul>
</li>
<li>创建→标准分区→挂载点：<strong>/</strong>→ ext4→使用全部可用空间<ul>
<li>如果已经有了三个分区，当分第四个分区的时候，<strong>第四个分区sda4默认为扩展分区</strong>，这样你就可以分更多的逻辑分区。</li>
<li>前面已经有sda1/2/3那么根分区就会变成sda5分区，第一个逻辑分区</li>
</ul>
</li>
</ul>
<p>以上为基本分区，安装好操作系统；还有高级分区，以后讨论。</p>
</li>
<li><p><strong>格式化</strong>（目的：写入文件系统）</p>
</li>
<li>询问将引导程序安装在哪里：用默认，直接下一步<ul>
<li>引导程序是什么？</li>
</ul>
</li>
<li><p><strong>询问需要安装哪些软件</strong>：选择basic server</p>
<ul>
<li>desktop：桌面，即图形界面（个人用户推荐）</li>
<li>minimal desktop：最小化桌面</li>
<li>minimal：最小化（服务器推荐，没有图形界面，linux占用资源更少，更多的空间给服务使用，服务越少，报错被攻击的可能性也越低；服务器的要求是稳定和安全；但是很多命名不会安装，要自己安装软件）</li>
<li>basic server：安装基本服务器的软件包（运维学习者推荐，常见的工具都会装好）</li>
<li>database server：安装数据库服务器的软件包</li>
<li>web server：安装网页服务器的软件包</li>
<li>virtual host ：虚拟主机</li>
<li>software development workstation : 软件开发工作站</li>
</ul>
<p>如果对linux非常熟悉，自己能够配置软件包环境，可以不用以上集成的软件包安装，可以自定义选择需要安装的软件，则会出现更加详细的linux软件选择。初学者不建议自定义软件安装。</p>
<p>如果第6步选择了英文安装，那么这里自定义中，在语言支持中勾选中文支持，否则之后要手工安装中文字体</p>
</li>
<li>软件包全部安装完成后，选择重新引导重启系统。注意：真实机再次重启后进入BIOS调整硬盘启动优先</li>
<li>启动完成，进入纯字符界面。安装完成</li>
<li>输入用户名root，输入密码（不会显示）</li>
</ol>
<h1 id="3-安装日志"><a href="#3-安装日志" class="headerlink" title="3. 安装日志"></a>3. 安装日志</h1><ul>
<li><strong>/root</strong> ： root用户的家目录，即每个用户的初始登陆位置 ~</li>
<li><strong>/root/install.log</strong> ：系统安装时，装了哪些软件包及其版本信息</li>
<li><strong>/root/install.log.syslog</strong> ：安装过程中留下的事件记录</li>
<li><strong>/root/anaconda-ks.cfg</strong> ：以Kickstart配置文件的格式记录安装过程中设置的选项信息<ul>
<li>kickstart指的是linux同时<strong>安装好几千台服务器时</strong>，如果完全手工安装很耗时，linux准备了<strong>无人值守安装</strong>，即不需要用户参与可以自动安装，安装过程依赖一个已经安装好的服务器，某个服务器作为模板，其他服务器跟它装的一样。依赖的配置文件就是 anaconda-ks.cfg</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux系统分区知识]]></title>
      <url>http://shengdeng.github.io/2016/05/15/Linux%E7%B3%BB%E7%BB%9F%E5%88%86%E5%8C%BA%E7%9F%A5%E8%AF%86/</url>
      <content type="html"><![CDATA[<h1 id="1-分区"><a href="#1-分区" class="headerlink" title="1. 分区"></a>1. 分区</h1><h2 id="1-1-磁盘分区"><a href="#1-1-磁盘分区" class="headerlink" title="1.1 磁盘分区"></a>1.1 磁盘分区</h2><p>系统分区也即磁盘分区，是使用分区编辑器(partition editor)在磁盘上划分几个逻辑部分。磁盘划分成数个分区(Patition)以后，不同类的目录与文件可以存储进不同的分区。</p>
<p>如果不分区会怎样？就像橱柜中没有分小隔间，所有东西都堆在一起，存放和取东西都十分不方便。同样的，磁盘数据量很大，不分区则读取数据和写入数据的效率会大大降低。例如，C盘放操作系统，D盘放娱乐资料。</p>
<h2 id="1-2-分区类型"><a href="#1-2-分区类型" class="headerlink" title="1.2 分区类型"></a>1.2 分区类型</h2><ul>
<li>主分区：最多只有4个</li>
<li>扩展分区：主分区有限，可以在主分区之上建立扩展分区<ul>
<li>一块硬盘上最多只能有1个</li>
<li>一块磁盘上，主分区＋扩展分区：总个数最多4个</li>
<li>扩展分区本身不能写入数据，也不能格式化；扩展分区唯一的作用就是包含更多逻辑分区。</li>
</ul>
</li>
<li>逻辑分区：可以正确的读写数据、格式化</li>
</ul>
<p>以上限制不是linux系统的限制，而是磁盘的限制，只要硬盘的结构不发生变化这种限制依然存在。</p>
<h1 id="2-格式化"><a href="#2-格式化" class="headerlink" title="2. 格式化"></a>2. 格式化</h1><p>硬盘正确分区之后还不能写入数据，还需要格式化才能读写数据。</p>
<p>格式化指的是高级格式化(还有低级格式化是硬盘的操作，不是操作系统的操作，不研究)，是系统的操作，又称逻辑格式化。</p>
<ul>
<li>格式化的目的是为了在硬盘中写入文件系统。<strong>两个工作：</strong><ul>
<li>格式化最主要的工作就是按照文件系统的规则将硬盘<strong>分割成等大小的数据块</strong>，数据块叫做Block。可以理解成柜子中加入隔断，但是隔断布局有一定的规则。EXT4的格式化是将分区变成一个一个等大小的数据块，默认的标准大小是4KB，假设一个数据是10KB，会拆成三个数据块，最后一个数据块内容有2KB但本身还是4KB，剩余的2KB空间不能再被占用；磁盘也不是连续保存这三个数据块的，而是分散在不同位置分别保存着三个数据块，所以windows中有磁盘碎片整理工具，原理就是保存文件的不同数据块尽量放在一起，这样更加利于数据的读取。</li>
<li><strong>建立一个inode列表</strong>。当用户需要读取文件时，根据表来查找文件放在那些数据块中。文件的编号成为i节点号(inode)，通过i节点号来找到文件的元数据信息，从而得知文件保存在哪几个数据块中，然后取出这些数据块，拼凑起来成为完整文件。</li>
</ul>
</li>
<li>文件系统<ul>
<li>windows可以识别的有FAT16、FAT32、NTFS</li>
<li>linux可以识别的有EXT2、EXT3、EXT4；新版本CentOs中默认使用ETX4</li>
</ul>
</li>
</ul>
<h1 id="3-设备文件名（系统自动识别）"><a href="#3-设备文件名（系统自动识别）" class="headerlink" title="3 设备文件名（系统自动识别）"></a>3 设备文件名（系统自动识别）</h1><h2 id="3-1-硬盘设备文件名"><a href="#3-1-硬盘设备文件名" class="headerlink" title="3.1 硬盘设备文件名"></a>3.1 硬盘设备文件名</h2><p>如果是windows，硬盘进行了分区→格式化→分配盘符→可以直接使用了。因为windows有完善的图形界面，可以通过图形界面看到分区在哪，所以可以直接分配盘符。</p>
<p>但是linux最早出现的时候没有图形界面，要让系统知道给哪个分区分配盘符，就必须先给每个硬件设备起一个设备文件名。<br><strong>分区→格式化→给设备（分区）起文件名→挂载→可以使用了</strong>。</p>
<p>注意：在linux中，所有硬件设备都是文件，比如 /dev下的文件都是设备文件名；<br>这些设备文件名是固定的，系统自动检测的，我们能够看懂就行。</p>
<table>
<thead>
<tr>
<th>硬件</th>
<th>设备文件名</th>
</tr>
</thead>
<tbody>
<tr>
<td>IDE接口的硬盘</td>
<td>/dev/hd[a-d]</td>
</tr>
<tr>
<td>SCSI/SATA/USB接口的硬盘</td>
<td>/dev/sd[a-p]</td>
</tr>
<tr>
<td>光驱</td>
<td>/dev/cdrom或/dev/sr0</td>
</tr>
<tr>
<td>打印机（USB）</td>
<td>/dev/usb/lp[0-15]</td>
</tr>
<tr>
<td>鼠标</td>
<td>/dev/mouse</td>
</tr>
</tbody>
</table>
<p>比如有一块硬盘就是sda，两块就是sda、sdb</p>
<ul>
<li>IDE硬盘最古老，理论的最高传输速度是133MB/s（基本淘汰）</li>
<li>SCSI硬盘与IDE同时代，更加昂贵理论传输速度200MB/s；主要用在服务器上，因为贵（基本淘汰）</li>
<li>SATA接口硬盘，小口的，现在使用最多，SATA三代理论传输速度可以达到500MB/s；目前不管服务器还是个人PC的硬盘接口一般都是SATA接口的硬盘（现在虚拟机模拟的接口也都是SATA接口硬盘）</li>
</ul>
<h2 id="3-2-分区文件名"><a href="#3-2-分区文件名" class="headerlink" title="3.2.分区文件名"></a>3.2.分区文件名</h2><p>硬盘有设备文件名，硬盘上的分区也要有设备文件名</p>
<p>规则：在硬盘文件名之后加分区号即可</p>
<ul>
<li>/dev/sda1 : 表示SATA硬盘中的第一个分区</li>
<li>分区方式一，在sda这块硬盘上：<ul>
<li>分了三个主分区文件命名为sda1、sda2、sda3</li>
<li>一个扩展分区文件命名sda4；里面有两个逻辑分区文件命名为sda5、sda6</li>
</ul>
</li>
<li>分区方式二（更常用），在sdb这个硬盘上：<ul>
<li>一个主分区，对应文件命名为sdb1</li>
<li>一个扩展分区，对应文件命名为sdb2；里面又分多个逻辑分区，分别命名为sdb5、sdb6、sdb7。<strong>注意：逻辑分区是从sdb5开始命名的，因为sdb1\2\3\4只能给主分区或者扩展分区使用，就算硬盘上没有分够四个分区，逻辑分区也不能用sdb1\2\3\4</strong>。所以sdb5一定是第一个逻辑分区。</li>
</ul>
</li>
</ul>
<p><img src="http://7xtorv.com1.z0.glb.clouddn.com/partition.PNG" alt=""></p>
<p>以上过程是系统自动识别，我们需要明白其命名规则就行。</p>
<h1 id="4-挂载"><a href="#4-挂载" class="headerlink" title="4 挂载"></a>4 挂载</h1><p>给<strong>分区</strong>指定<strong>挂载点</strong>的过程称为<strong>挂载</strong>（挂载点就是windows中的盘符的说法，windos中用C\D\E\F作为盘符，linux中用空的<strong>目录名称作为挂载点</strong>）。</p>
<ul>
<li>必须分区<ul>
<li><strong> /（根分区）</strong>：最高一级目录，一定要给他一个分区（也即 必须有一个分区必须以/作为挂载点），否则/目录下数据没地方保存</li>
<li><strong>swap分区</strong>（[美]死哇破）：交换分区，可以理解为硬盘上的虚拟内存，当内存不够用的时候，可以用这部分磁盘空间来当内存用。内存2倍，不超过2Gb，给更多也不会带来性能的提升反而占磁盘空间。</li>
</ul>
</li>
<li>推荐分区<ul>
<li><strong>/boot</strong>：启动分区，大约200Mb。任何一个操作系统正常启动都必须有一定的剩余空间，如果磁盘被写满，会导致linux启动不起来，所以一般都会给/boot单独一个分区，因为/boot是专门保存系统启动时需要的数据。</li>
</ul>
</li>
</ul>
<p><img src="http://7xtorv.com1.z0.glb.clouddn.com/1111.PNG" alt=""></p>
<p>注意：由于/目录下的子目录可以分配独立的分区，所以，如果往/boot中写入数据，实际上写入sda磁盘的第一个分区中。从linux系统上看，/boot是/的子目录；从硬盘上来看，每个目录都可以有独立的硬盘空间（分区）。</p>
<h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h1><ol>
<li><strong>分区</strong>：把大硬盘分为小的分区</li>
<li><strong>格式化</strong>：写入文件系统</li>
<li><strong>分区设备文件名</strong>：给每个分区（命名）定义设备文件名</li>
<li><strong>挂载</strong>：给每个分区分配挂载点（即目录）</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[VMware虚拟机的使用总结]]></title>
      <url>http://shengdeng.github.io/2016/05/14/VMware%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h1 id="VMware简介"><a href="#VMware简介" class="headerlink" title="VMware简介"></a>VMware简介</h1><p>VMware主要特点：</p>
<ol>
<li>在一台PC上运行多个系统</li>
<li>本机系统可以与虚拟机系统网络通信</li>
<li>可以设定并且随时修改虚拟机操作系统的硬件配置</li>
</ol>
<p>VMware安装：下一步下一步即可</p>
<h1 id="新建一个虚拟机的向导步骤"><a href="#新建一个虚拟机的向导步骤" class="headerlink" title="新建一个虚拟机的向导步骤"></a>新建一个虚拟机的向导步骤</h1><ol>
<li>新建虚拟机</li>
<li>选择典型安装</li>
<li>安装来源选择以后安装操作系统<ul>
<li>光盘安装，系统刻成光盘了，比较麻烦；</li>
<li>安装镜像文件，linux的镜像文件是从官方网站提前免费下载的，如果选这个下一步之后所有安装linux的过程都不需要你参与了，而且默认装的是最小化系统，；</li>
<li>创建一个空白硬盘，以后再安装操作系统，可以自己配置硬件。</li>
</ul>
</li>
<li>选择一个将要安装的系统，选linux，选择linux版本，linux版本众多，如果没有选择一个内核版本即可</li>
<li>给新建的虚拟机起一个名字，与其他虚拟机区分开；给虚拟机选择一个文件保存的位置目录，不要选择C盘就可以，避免占用C盘空间，今后不需要这个虚拟机时，直接删除这个目录就可以。(编辑→首选项 中可以更改默认位置)</li>
<li>指定磁盘大小。默认是20GB，足够了，虚拟机会根据你的使用情况分配磁盘，不会一次性分配20GB，这里的20GB指，从linux内查看磁盘大小为20GB</li>
<li>完成</li>
</ol>
<h1 id="修改虚拟机硬件设置"><a href="#修改虚拟机硬件设置" class="headerlink" title="修改虚拟机硬件设置"></a>修改虚拟机硬件设置</h1><p><code>选中虚拟机→虚拟机→设置</code>或者点击<code>编辑虚拟机设置</code></p>
<ul>
<li>内存：注意虚拟机的内存不能超过真实机的一半。对CentOs以上的系统来讲，要想看到图形界面，内存的最低要求是628M</li>
<li>处理器：如果真实机有多个处理器，可以选择处理器数量；处理器核数也可以根据真实机的核数选择</li>
<li>硬盘：可以选择硬盘后点击<code>添加</code>下一步下一步添加多块硬盘</li>
<li>光驱CD/DVD：右边连接设置选择<code>使用ISO映像文件</code>然后选择 下载的系统ISO镜像目录(如果有两个ISO镜像，选择DVD1保存了主体数据，DVD2保存了一些安装包)，此步相当于把光盘放进光驱；设备状态中的<code>已连接</code>没有打勾，表示光驱没有接电源，即光盘放进光驱里也不会运行，启动虚拟机后，如果发现光驱无法读取，可以检查<code>已连接</code>是否打勾</li>
<li>软盘：可以删除，省得启动时还会检测拖慢启动速度，像虚拟打印机、声卡都可以删掉</li>
<li>网络适配器：网络连接如下方式可选<ul>
<li>桥接模式</li>
<li>NAT模式</li>
<li>Host-only模式</li>
</ul>
</li>
</ul>
<h1 id="VMware使用技巧"><a href="#VMware使用技巧" class="headerlink" title="VMware使用技巧"></a>VMware使用技巧</h1><h2 id="虚拟机快照"><a href="#虚拟机快照" class="headerlink" title="虚拟机快照"></a>虚拟机快照</h2><p>拍摄此虚拟机快照<code>小时钟</code>：把虚拟机当前状态保存下来，以后出现了问题可以恢复到此处，不需要重装系统；或者你不想启动虚拟机花费时间，可以在启动后创建一个快照，以后开机直接让VMware读取快照，就可以很快到开机后的状态；注意快照会占用硬盘资源</p>
<ul>
<li>名称：比如<code>初始安装的状态</code></li>
<li>描述：描述下这个状态</li>
</ul>
<p>恢复快照<code>小扳手</code>：选中快照，点击<code>转到</code>就可恢复</p>
<p>注意：真正的服务器中没有快照，工作时尽量减少误操作</p>
<h2 id="克隆"><a href="#克隆" class="headerlink" title="克隆"></a>克隆</h2><p>说明：克隆出来一个和当前虚拟机一样的虚拟机，建立集群时很方便；好处是，克隆机并不像创建的虚拟机一样占用那么大的空间，而是克隆机实际用了多少空间就会占用多少空间，克隆机不是一个完整的计算机，可能只有几十兆，于是有了多台计算机，又占用的资源少</p>
<p>注意：克隆机只是真实机的镜像，如果删掉了原始机，克隆机不能使用了；克隆机删除，不影响原始机使用</p>
<p><code>虚拟机→管理→克隆</code>：设置向导</p>
<ol>
<li>选择克隆源<ul>
<li>虚拟机当前状态</li>
<li>某个快照</li>
</ul>
</li>
<li>克隆类型<ul>
<li>链接克隆：克隆机占的资源很少(推荐这个)</li>
<li>完整克隆：相当于复制一个原始机，两个计算机占用同样的大小</li>
</ul>
</li>
<li>克隆机起名字，选择保存的位置，与原始机的目录放一起</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark与MapReduce对比测评]]></title>
      <url>http://shengdeng.github.io/2016/05/13/Spark%E4%B8%8EMapReduce%E5%AF%B9%E6%AF%94%E6%B5%8B%E8%AF%84/</url>
      <content type="html"><![CDATA[<h1 id="基本原理方面"><a href="#基本原理方面" class="headerlink" title="基本原理方面"></a>基本原理方面</h1><p><em>MapReduce</em></p>
<ul>
<li>基于磁盘的大数据批量处理系统</li>
<li>Map和Reduce阶段都是往磁盘读写</li>
</ul>
<p><em>Spark</em></p>
<ul>
<li>基于RDD(弹性分布式数据集)数据处理。RDD可以显示地(即可指定)存储到磁盘和内存中</li>
</ul>
<p><strong>总结</strong>：Spark比MR更灵活，更快速</p>
<h1 id="模型方面"><a href="#模型方面" class="headerlink" title="模型方面"></a>模型方面</h1><p><em>MapReduce</em></p>
<ul>
<li>可以处理<strong>超大规模</strong>的数据</li>
<li>适合日志分析挖掘等</li>
<li>较少的迭代的长任务需求</li>
<li>结合了数据的分布式计算、存储</li>
</ul>
<p><em>Spark</em></p>
<ul>
<li>适合海量数据，但<strong>不适合超大规模</strong>(毕竟数据放在内存中)</li>
<li>适合<strong>数据挖掘</strong>(迭代中间结果放在内存中，迭代时很快)、机器学习等多轮迭代式计算任务</li>
<li>在处理上是分布式的，但在存储上数据来源于HDFS等第三方系统</li>
</ul>
<p><strong>总结</strong>：Spark在数据挖掘处理方面比MapReduce优越，但在大规模数据处理上，目前MR更适合。</p>
<h1 id="容错性方面"><a href="#容错性方面" class="headerlink" title="容错性方面"></a>容错性方面</h1><p>数据的容错性、节点的容错性<br><em>MapReduce</em></p>
<ul>
<li>数据的读和写都是往HDFS上，HDFS有很强的数据容错的措施</li>
</ul>
<p><em>Spark</em></p>
<ul>
<li>容错性建立在RDD的Linage上，重新构造RDD</li>
</ul>
<p><strong>总结</strong>：Spark在容错性方面并不比MapReduce优越</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[前沿追踪：伯克利数据分析堆栈]]></title>
      <url>http://shengdeng.github.io/2016/05/12/%E5%89%8D%E6%B2%BF%E6%8A%80%E6%9C%AF%E8%BF%BD%E8%B8%AA%EF%BC%9A%E4%BC%AF%E5%85%8B%E5%88%A9%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%A0%86%E6%A0%88/</url>
      <content type="html"><![CDATA[<p>伯克利数据分析堆栈(BDAS, the Berkeley Data Analytics Stack)是由AMPLab实验室提供的，以Spark Core计算引擎为核心的，一套完整应对各种大数据处理场的Spark生态系统。伯克利数据分析堆栈也会随着开源软件的发展不断向前演变。 <a href="https://amplab.cs.berkeley.edu/projects" target="_blank" rel="external">AMPLab实验室</a>作为推动Spark生态向前发展的主要力量，其提供的伯克利数据分析堆栈非常值得技术人员持续关注。【草稿版。。。】<br><a id="more"></a></p>
<h1 id="Spark历史回顾"><a href="#Spark历史回顾" class="headerlink" title="Spark历史回顾"></a>Spark历史回顾</h1><p><strong>2009年</strong>Spark作为一个研究项目在加州大学伯克利分校(UC Berkeley)的RAD实验室(AMPlab的前身)诞生。当时Spark的设计是为了弥补Hadoop MR在迭代计算、交互计算、计算时间方面效率底下的缺陷，因此Spark天生具备以下特性</p>
<ul>
<li>可交互式查询</li>
<li>迭代计算</li>
<li>内存式存储</li>
<li>高效容错</li>
</ul>
<p><strong>2010年3月</strong>Spark项目开源。</p>
<p><strong>2011年</strong>，AMPLab开始基于Spark开发更高层的组件，比如Spark Streaming。有AMP开发的这些基于Spark Core组件和其他组件一起被称为<a href="https://amplab.cs.berkeley.edu/software/" target="_blank" rel="external">伯克利数据分析堆栈</a>。分析栈随着技术的发展不断扩张和演变。</p>
<p><strong>2013年6月</strong>，Spark项目正式交给Apache基金会，目前已成为顶级项目。从此，Spark飞速发张。</p>
<h1 id="伯克利数据分析堆栈"><a href="#伯克利数据分析堆栈" class="headerlink" title="伯克利数据分析堆栈"></a>伯克利数据分析堆栈</h1><p>BDAS的所有组件如下图(2016年版，如<a href="https://amplab.cs.berkeley.edu/software/" target="_blank" rel="external">官网</a>有更新欢迎留言告诉我)。其中，蓝色和绿色的模块所代表的组件完全开源可用，点击链接可进入对应项目的主页下载或了解更多。<br><img src="http://7xtorv.com2.z0.glb.clouddn.com/%E6%8D%95%E8%8E%B7.PNG" alt=""></p>
<p><strong>BDAS主要包括三层：存储层、资源管理层和数据处理层</strong>。AMP自己的项目中除了处理层的Spark,还有管理层的Mesos，存储层的Tachyon；考虑到Hadoop生态圈技术在某些情况下数据处理的会更好，作为Spark的互补技术，Spark对Hadoop支持且能够和Hadoop整合在一起。因此，BDAS表示的Spark生态系统还包括Hadoop的技术。下面详谈各个模块的细节。</p>
<h2 id="存储层"><a href="#存储层" class="headerlink" title="存储层"></a>存储层</h2><p>Tachyon是一个基于内存的文件系统，在一个集群中有多个框架，这些框架的数据可能需要共享，如果把数据直接放在内存中共享时更快。Tachyon是华人弄得项目已经开源。</p>
<h2 id="资源管理层"><a href="#资源管理层" class="headerlink" title="资源管理层"></a>资源管理层</h2><p>Mesos是资源管理层的，使多框架可以分享同一个集群的资源。这里你只要有一个集群就可以了，而不需要有很多个，能够在多个框架上用。好处是什么？资源节省效率更高，并且更容易共享数据。</p>
<h2 id="处理层"><a href="#处理层" class="headerlink" title="处理层"></a>处理层</h2><p>Spark core是执行引擎，它有两个特点，它可以容错，内存存储效率也很高。在时效节点上它可以对数据进行重构，并且它有更强大的模型，也更快，比Hadoop MapReduce快上100倍。对于同样一个应用来说它写的代码量要比Hadoop MapReduce少2到5倍，并且它也支持互动计算。也就是说它对内存的利用效率更高，能更快得到响应，并且也可以通过内存共享数据。在Spark Core上面有很多个框架：</p>
<h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p>用Spark的功能做快速的计算。流式计算更适用于实时计算的场景。我们可以用它处理不同节点的计算，所有的这些特性实际上在其他类似Spark Streaming技术上不能实现的。这里我们支持批量流的计算。</p>
<h3 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h3><p>用于机器学习的高质量库</p>
<h3 id="Graphx"><a href="#Graphx" class="headerlink" title="Graphx"></a>Graphx</h3><p>这个Graphx可以整合数据和图并行计算，就是对数据的平行计算以及表格的平行计算进行整合，提供了非常高的抽象等级。同时，它还借鉴了Apache Spark的纠错功能。</p>
<h3 id="BlinkDB"><a href="#BlinkDB" class="headerlink" title="BlinkDB"></a>BlinkDB</h3><p>使用抽样方法在<strong>性能</strong>和<strong>准确性</strong>之间做平衡，服务器响应速度非常快。用Sql查询，BlinkDB可以在一定的响应时间内进行查询，目前仍在开发过程中，将来是Spark的一个非常重要的部分。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark四大特性]]></title>
      <url>http://shengdeng.github.io/2016/05/10/Spark%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7/</url>
      <content type="html"><![CDATA[<p><a href="http://spark.apache.org/" target="_blank" rel="external">Apache Spark™</a> is a fast and general engine for large-scale data processing.<br>Apache Spark™是一个<strong>快速的</strong>、<strong>通用的</strong>，针对于<strong>大数据集</strong>的<strong>数据处理分析</strong>的计算引擎（框架）。</p>
<ul>
<li>Hadoop:数据存储（HDFS）与分析(MR)</li>
<li>Spark：数据分析</li>
</ul>
<p>在数据处理上Spark与MR比较，主要有如下四点优势特性</p>
<a id="more"></a>
<h1 id="快速-Speed"><a href="#快速-Speed" class="headerlink" title="快速(Speed)"></a>快速(Speed)</h1><p>与Hadoop MapReduce比较，内存中运行快100倍以上，磁盘中运行快10倍以上。</p>
<p>主要原因：</p>
<ol>
<li>Spark使用有向无环图DAG（任务划为一个一个流程图）</li>
<li>Spark在内存中运行（MR map输出结果仿真磁盘，reduce读数据从磁盘读，磁盘读写速度依赖网络IO、磁盘速度）</li>
</ol>
<h1 id="易使用-Ease-of-Use"><a href="#易使用-Ease-of-Use" class="headerlink" title="易使用(Ease of Use)"></a>易使用(Ease of Use)</h1><p>spark编程简单，支持scala、java、python，尤其scala和python代码很简洁，代码量少：</p>
<ul>
<li>spark提供了<strong>80个以上的操作</strong>（除了map\reduce还有很多操作）使得编程简单。</li>
<li>提供了可交互的 <strong>scala shell</strong> 和 python shell</li>
</ul>
<p>MR编程则十分麻烦，虽然有模板可以套用，但代码总体庞大；MR只有map和reduce两个操作，一个业务总要想办法拆成（多个）map和reduce，甚至多个job，</p>
<h1 id="通用性-Generality"><a href="#通用性-Generality" class="headerlink" title="通用性(Generality)"></a>通用性(Generality)</h1><p>Spark框架可以视为core，建立在spark运算框架上，还有其他专用框架（像MR上建立的框架：MapReduce难写有了Hive框架、机器学习Mahont框架、Pig、图形计算Giraph），Spark上的框架相当于一些Jar包库，不需要像Hive、Mahont要另外部署配置。</p>
<p>Spark powers（为…提供动力） a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. You can combine these libraries seamlessly in the same application.<br>Spark内核框架驱动一堆高级工具库包括SQL，DataFrames, 机器学习库MLlib, GraphX, and Spark Streaming。 你只要部署好了spark，这些类库就直接可以用，即<strong>一站式的大数据处理解决方案(One stack to rule all!)</strong>。因此不需要为了各种业务而搭建平台。（Hadoop平台上要为了各种业务搭建一堆东西，比如Hive、Pig等等）</p>
<p>总之，基于Spark core上的大一统软件栈的优点：</p>
<ul>
<li>spark下层core的改进优化时，上层程序库也自动获得性能提升；</li>
<li>运行整个软件栈的代价小（包括部署、维护、测试、支持），不需要运行多套独立的软件系统；</li>
<li>能够无缝整合不同处理模型的应用（如机器学习的应用和SQL实时查询结果数据的应用在Spark上同时进行）</li>
</ul>
<h1 id="运行在各个地方-Runs-Everywhere"><a href="#运行在各个地方-Runs-Everywhere" class="headerlink" title="运行在各个地方(Runs Everywhere)"></a>运行在各个地方(Runs Everywhere)</h1><p>多种运行方式</p>
<ul>
<li>单独部署：standalone</li>
<li>Yarn</li>
<li>类似于yarn的资源管理框架Mesos</li>
<li>云端：EMC、EC2</li>
</ul>
<p>运行数据来源</p>
<ul>
<li><strong>Spark可以支持任何实现了Hadoop API的存储系统</strong>(Access data in HDFS, Cassandra, HBase, Hive, Tachyon, and any Hadoop data source.)。Spark可以从存储在Hadoop分布式文件系统（HDFS）中的任何文件，或其他Hadoop API支持的存储系统（如本地文件系统，Amazon S3， Cassandra， Hive，HBase等）创建分布式数据集。</li>
<li><strong>Hadoop对Spark来说不是必须的</strong>，Spark支持文本文件、序列文件、Avro、Parquet，以及任何其他Hadoop的输入格式。</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Kafka官方文档中文版：第1章 Getting Started]]></title>
      <url>http://shengdeng.github.io/2016/05/06/Kafka%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E4%B8%AD%E6%96%87%E7%89%88%EF%BC%9A%E7%AC%AC1%E7%AB%A0GettingStarted/</url>
      <content type="html"><![CDATA[<p><a href="http://kafka.apache.org/documentation.html" target="_blank" rel="external">Kafka版本：0.9.0</a></p>
<p>正在持续更新中…</p>
<a id="more"></a>
<h1 id="1-1-引言-Introduction"><a href="#1-1-引言-Introduction" class="headerlink" title="1.1 引言(Introduction)"></a>1.1 引言(Introduction)</h1><p>Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.<br>What does all that mean?</p>
<p>First let’s review some basic messaging terminology:</p>
<ul>
<li>Kafka maintains feeds of messages in categories called topics.</li>
<li>We’ll call processes that publish messages to a Kafka topic producers.</li>
<li>We’ll call processes that subscribe to topics and process the feed of published messages consumers..</li>
<li>Kafka is run as a cluster comprised of one or more servers each of which is called a broker.</li>
</ul>
<p>So, at a high level, producers send messages over the network to the Kafka cluster which in turn serves them up to consumers like this:</p>
<h2 id="Topics-and-Logs"><a href="#Topics-and-Logs" class="headerlink" title="Topics and Logs"></a>Topics and Logs</h2><p>Let’s first dive into the high-level abstraction Kafka provides—the topic.<br>A topic is a category or feed name to which messages are published. For each topic, the Kafka cluster maintains a partitioned log that looks like this:</p>
<p>Each partition is an ordered, immutable sequence of messages that is continually appended to—a commit log. The messages in the partitions are each assigned a sequential id number called the offset that uniquely identifies each message within the partition.<br>The Kafka cluster retains all published messages—whether or not they have been consumed—for a configurable period of time. For example if the log retention is set to two days, then for the two days after a message is published it is available for consumption, after which it will be discarded to free up space. Kafka’s performance is effectively constant with respect to data size so retaining lots of data is not a problem.</p>
<p>In fact the only metadata retained on a per-consumer basis is the position of the consumer in the log, called the “offset”. This offset is controlled by the consumer: normally a consumer will advance its offset linearly as it reads messages, but in fact the position is controlled by the consumer and it can consume messages in any order it likes. For example a consumer can reset to an older offset to reprocess.</p>
<p>This combination of features means that Kafka consumers are very cheap—they can come and go without much impact on the cluster or on other consumers. For example, you can use our command line tools to “tail” the contents of any topic without changing what is consumed by any existing consumers.</p>
<p>The partitions in the log serve several purposes. First, they allow the log to scale beyond a size that will fit on a single server. Each individual partition must fit on the servers that host it, but a topic may have many partitions so it can handle an arbitrary amount of data. Second they act as the unit of parallelism—more on that in a bit.</p>
<h2 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h2><p>The partitions of the log are distributed over the servers in the Kafka cluster with each server handling data and requests for a share of the partitions. Each partition is replicated across a configurable number of servers for fault tolerance.<br>Each partition has one server which acts as the “leader” and zero or more servers which act as “followers”. The leader handles all read and write requests for the partition while the followers passively replicate the leader. If the leader fails, one of the followers will automatically become the new leader. Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster.</p>
<h2 id="Producers"><a href="#Producers" class="headerlink" title="Producers"></a>Producers</h2><p>Producers publish data to the topics of their choice. The producer is responsible for choosing which message to assign to which partition within the topic. This can be done in a round-robin fashion simply to balance load or it can be done according to some semantic partition function (say based on some key in the message). More on the use of partitioning in a second.<br>Consumers</p>
<p>Messaging traditionally has two models: queuing and publish-subscribe. In a queue, a pool of consumers may read from a server and each message goes to one of them; in publish-subscribe the message is broadcast to all consumers. Kafka offers a single consumer abstraction that generalizes both of these—the consumer group.<br>Consumers label themselves with a consumer group name, and each message published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.</p>
<p>If all the consumer instances have the same consumer group, then this works just like a traditional queue balancing load over the consumers.</p>
<p>If all the consumer instances have different consumer groups, then this works like publish-subscribe and all messages are broadcast to all consumers.</p>
<p>More commonly, however, we have found that topics have a small number of consumer groups, one for each “logical subscriber”. Each group is composed of many consumer instances for scalability and fault tolerance. This is nothing more than publish-subscribe semantics where the subscriber is cluster of consumers instead of a single process.</p>
<p>A two server Kafka cluster hosting four partitions (P0-P3) with two consumer groups. Consumer group A has two consumer instances and group B has four.<br>Kafka has stronger ordering guarantees than a traditional messaging system, too.</p>
<p>A traditional queue retains messages in-order on the server, and if multiple consumers consume from the queue then the server hands out messages in the order they are stored. However, although the server hands out messages in order, the messages are delivered asynchronously to consumers, so they may arrive out of order on different consumers. This effectively means the ordering of the messages is lost in the presence of parallel consumption. Messaging systems often work around this by having a notion of “exclusive consumer” that allows only one process to consume from a queue, but of course this means that there is no parallelism in processing.</p>
<p>Kafka does it better. By having a notion of parallelism—the partition—within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes. This is achieved by assigning the partitions in the topic to the consumers in the consumer group so that each partition is consumed by exactly one consumer in the group. By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances. Note however that there cannot be more consumer instances in a consumer group than partitions.</p>
<p>Kafka only provides a total order over messages within a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications. However, if you require a total order over messages this can be achieved with a topic that has only one partition, though this will mean only one consumer process per consumer group.</p>
<h2 id="Guarantees"><a href="#Guarantees" class="headerlink" title="Guarantees"></a>Guarantees</h2><p>At a high-level Kafka gives the following guarantees:<br>Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.<br>A consumer instance sees messages in the order they are stored in the log.<br>For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log.<br>More details on these guarantees are given in the design section of the documentation</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark编程指南中文版]]></title>
      <url>http://shengdeng.github.io/2016/05/06/Spark%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E4%B8%AD%E6%96%87%E7%89%88/</url>
      <content type="html"><![CDATA[<p><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">Spark版本：1.6.1</a></p>
<p>正在持续更新中…</p>
<a id="more"></a>
<h1 id="1-概述-Overview"><a href="#1-概述-Overview" class="headerlink" title="1 概述(Overview)"></a>1 概述(Overview)</h1><p>总体来讲，每一个Spark驱动程序应用都由一个驱动程序组成，该驱动程序包含一个由用户编写的main方法，该方法会在集群上并行执行一些列并行计算操作。Spark最重要的一个概念是弹性分布式数据集，简称RDD（resilient distributed dataset ）。RDD是一个数据容器，它将分布在集群上各个节点上的数据抽象为一个数据集，并且RDD能够进行一系列的并行计算操作。可以将RDD理解为一个分布式的List，该List的数据为分布在各个节点上的数据。RDD通过读取Hadoop文件系统中的一个文件进行创建，也可以由一个RDD经过转换得到。用户也可以将RDD缓存至内存，从而高效的处理RDD，提高计算效率。另外，RDD有良好的容错机制。</p>
<p>Spark另外一个重要的概念是共享变量（shared variables）。在并行计算时，可以方便的使用共享变量。在默认情况下，执行Spark任务时会在多个节点上并行执行多个task，Spark将每个变量的副本分发给各个task。在一些场景下，需要一个能够在各个task间共享的变量。Spark支持两种类型的共享变量：</p>
<ul>
<li><p>广播变量（broadcast variables）：将一个只读变量缓存到集群的每个节点上。例如，将一份数据的只读缓存分发到每个节点。</p>
</li>
<li><p>累加变量（accumulators）：只允许add操作，用于计数、求和。</p>
</li>
</ul>
<h1 id="2-引入Spark（Linking-with-Spark"><a href="#2-引入Spark（Linking-with-Spark" class="headerlink" title="2 引入Spark（Linking with Spark)"></a>2 引入Spark（Linking with Spark)</h1><p>在Spark 1.6.0上编写应用程序，支持使用Scala 2.10.X、Java 7+、Python 2.6+、R 3.1+。如果使用Java 8，支持lambda表达式（lambda expressions）。</p>
<p>在编写Spark应用时，需要在Maven依赖中添加Spark，Spark的Maven Central为：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">groupId = org.apache.spark</span><br><span class="line">artifactId = spark-core_2.10</span><br><span class="line">version = 1.6.1</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Guide、Manual和Tutorials的区别]]></title>
      <url>http://shengdeng.github.io/2016/05/06/Guide%E3%80%81Manual%E5%92%8CTutorial%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      <content type="html"><![CDATA[<p>开源技术的官方网站提供了许多指导文档，理解他们的真正含义，会有事半功倍的效果。</p>
<a id="more"></a>
<h2 id="Tutorials"><a href="#Tutorials" class="headerlink" title="Tutorials"></a>Tutorials</h2><p>Tutorial </p>
<ul>
<li>n. 个别指导</li>
<li>对一个关键点的指导文档，一个Tutorial只教会一个关键点</li>
<li>例如，如果你在<a href="https://class.coursera.org/progfun-005/wiki/ScalaStyleGuide" title="scala公开课" target="_blank" rel="external">scala公开课</a>的wiki（实现团队协作编辑网页和文档共享的网站）上看到Eclipse tutorial，那么这个教程将教你如何在eclipse上开发scala项目，有时点到Hello World为止，仅此而已，不会介绍更多的eclipse使用方法。</li>
</ul>
<p>Tutorials</p>
<ul>
<li>Tutorial的复数形式，即众多的知识点教程的集合</li>
<li>例如，scala官网文档<a href="http://docs.scala-lang.org/tutorials/?_ga=1.81127127.178612781.1460424230" target="_blank" rel="external">Scala Tutorials</a>的目录，罗列了许多scala的关键知识点的指导教程，不像Guide会分章节。</li>
</ul>
<p>所以，Tutorials文档有点对系统中各个知识点逐个击破的意味。</p>
<h2 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a>Guide</h2><p>Guide</p>
<ul>
<li>n. 指南；向导；入门书</li>
<li>指导作用且对技术的理解有帮助的文档，可以理解为国内常用的“从入门道精通”，通常先解释技术原理，然后指导使用方法，适合学习</li>
</ul>
<p>官网的Guide是很重要的学习资料，随软件或语言版本及时更新，也是最权威的指南！英语不好的同学可以google Guide的中文翻译，比如，找“Spark Programming Guide”中文翻译，可以搜 “Spark” “编程指南”，但是一定要注意翻译对应的版本是否和你的软件版本一致。</p>
<h2 id="Manual"><a href="#Manual" class="headerlink" title="Manual"></a>Manual</h2><p>Manual</p>
<ul>
<li>n. 使用手册</li>
<li>工作时放在手边随时查阅的文档，内容全面，语言精炼高效，适合忘记某个参数时快速查阅使用方法</li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[自品牌也应该有的信念：如果不能成为第一，一开始就不要碰]]></title>
      <url>http://shengdeng.github.io/2016/05/04/%E8%87%AA%E5%93%81%E7%89%8C%E4%B9%9F%E5%BA%94%E8%AF%A5%E6%9C%89%E7%9A%84%E4%BF%A1%E5%BF%B5%EF%BC%9A%E5%A6%82%E6%9E%9C%E4%B8%8D%E8%83%BD%E6%88%90%E4%B8%BA%E7%AC%AC%E4%B8%80%EF%BC%8C%E4%B8%80%E5%BC%80%E5%A7%8B%E5%B0%B1%E4%B8%8D%E8%A6%81%E7%A2%B0/</url>
      <content type="html"><![CDATA[<p><strong>读书笔记：《记事本圆梦计划》PART 7</strong></p>
<blockquote>
<p>实习以后，最大的感触的就是职业生涯已经开始了，本来回学校想好好再享受一把，却十分不安，不想像本科的时候一样，因为要接触社会而感到焦虑不堪。或许，最近书看多了，把脑袋里的小怪兽调教的不安分，总吵吵着要干这个、学学那个。这篇读书笔记就是在小怪兽的吵吵下写的，之后搭建好了个人博客，再写计划、任务入嘀嗒收集箱，终于，有了滑板鞋 天黑都不怕 一步两步 一步两步…摩擦 摩擦 我给自己打着 节拍…</p>
</blockquote>
<p>最近想要建立自品牌形象，想着要经营微博，facebook，公众号，想着搭建个人主页博客，写技术博客，一边摸索一边又觉得毫无头绪，处在工作起步阶段的我还要不断学习新专业技能，同时又经营社交网络，会不会力不从心得不偿失？要发布哪些东西到社交网站？技术博客要有什么主题内容？才能吸引人，我的自品牌要建立怎样的形象？自品牌对我的人生有什么影响？还有很多要思考的问题。小确信，《记事本》中的内容给了我一丢丢的启发。</p>
<a id="more"></a>
<p><strong>正文</strong></p>
<p>PART 7中熊谷作为企业家，介绍了他管理公司的一些基本但很重要的观点。从这些观点中得到了一些启发，产生了一点联想，让我对自己将要建立的自品牌有了一个初步的想法。</p>
<h3 id="传递梦想：让一切建立在感动之上"><a href="#传递梦想：让一切建立在感动之上" class="headerlink" title="传递梦想：让一切建立在感动之上"></a>传递梦想：让一切建立在感动之上</h3><blockquote>
<p>经营公司最大的目的不是追求盈利，而是让经营者的梦想及感动，与<strong>员工、顾客、股东、合作厂商等人</strong>共同分享。</p>
</blockquote>
<p>熊谷用反证解释了这句话，如果老板不与任何人分享感动或者梦想，一味以营收来经营事业的话：</p>
<ul>
<li>员工：为钱工作→更高的薪水→跳槽</li>
<li>顾客：看价格→买更便宜的</li>
<li>股东：看报表→不理想→撤资</li>
</ul>
<p>如此的残酷都是因为只看眼前利益。所以说，要让与你的事业有利害关系的角色，与你有一致的梦想，甚至能创造出“笑容”和“感动”的想法，与你一起实现梦想（经营事业），那么他们不会残酷地离开你，因为<strong>你们之间获得了金钱无法换来的感动</strong>，同样也能带动营销与获利。</p>
<p>读完书里的这节内容，很容易联想到一些出色的大公司，当你听到他的品牌名字的时候，脑海里马上浮现他们的CEO阐述梦想的画面，和那些实践梦想的优秀产品，而且你一定会为这些与你认同的梦想买单。</p>
<p>同样的，在自品牌这个话题上，想一个场景，没有传递梦想与感动的博客是怎样的</p>
<ul>
<li>没有梦想和原则→自己写的文章说的话都打脸→不可靠称不上品牌</li>
<li>偶尔好文，博文内容良莠不齐→读者看一次而已→不会长久关注</li>
<li>读者的评论不回复，觉得浪费时间→读者觉得是一个“死”的博客→无法获得感动→无法建立联系</li>
</ul>
<p>好的博客是有灵魂的，可以互动的，读者阅读一篇文章之后会去读其他文章，会去好奇作者是什么人，会去找这个博主的联系方式。更厉害的是，在谈论某个领域时，读者可以马上想到你；又或他觉得你的博客值得分享给更多的人。这些都是因为博主在传递信息的同时，让读者感动。</p>
<p>因此，让自品牌能够传递梦想与感动也很重要。<strong>让“读者”能够感同你的梦想，并且从你做的事中得到感动，最后你将从别人那里收获感动</strong>。接下来就是要想清楚，<strong>我有什么样的梦想，在这个梦想下做出什么事能让大家感动</strong>。从小处可以做的，写一篇技术教程博客，能够把技术细节，把坑都解释透彻，能让读者觉得收获很大；博客的文章不是随意的，保证不要误导和出现错误，表达简洁清晰，这些也许就能传递一点点感动；回复读者的评论，和读者成为网络社交的好友，在专业领域和读者互动，也让读者来创造“笑容”和“感动”的想法。请想清楚，有哪些需要做到的细节。</p>
<h3 id="彻底到细节的执行力"><a href="#彻底到细节的执行力" class="headerlink" title="彻底到细节的执行力"></a>彻底到细节的执行力</h3><p>说到细节，熊谷对细节的要求已经到了极致的程度，他规定，桌椅在地板上有记号，若是移动了可以马上归位，甚至要求厕所一定要保持整洁，维持洗手台的干爽。他的观点是</p>
<blockquote>
<p>力量存在于细节中。对于小事不随便的公司，才能抓住厂商或是客人的心。</p>
</blockquote>
<p>如此<strong>彻底到细节的执行力</strong>，可以给人留下印象：</p>
<ul>
<li>清爽整洁，业务效率一定会很高</li>
<li>可靠，不允许出现差错</li>
</ul>
<p>打造自品牌的时候亦需要彻底的执行力，品牌创建初期，大家不了解你的时候，如果一个不注意的细节给别人造成先入为主的差印象就很麻烦了。那要如何培养彻底的执行力：</p>
<ol>
<li>首先，制定基本的行为规范：    <ul>
<li><strong>事无巨细的行为规范</strong>。可以让我们对每件事都有迅速的判断力（价值观），不会把时间浪费在纠结上；终极的效果是，让人感觉到发布的东西有统一的灵魂，能感觉是出自某一个人之手。</li>
<li><strong>有个性的行为规范</strong>。幽默？睿智？当灵魂有了个性，就增加了识别度，人们可以从茫茫人海中把你找出来。就像好声音评委说的：你的声音很有特色，我喜欢，加入我的战队吧。</li>
</ul>
</li>
<li>执行！执行！执行！<ul>
<li>《记事本圆梦计划》这本书，就是教你如何让梦想落地，将计划具体化数字化然后执行。《番茄工作法图解》、《小强升职记》等时间管理类的书都会教你如何高效的执行。看完书后，在执行过程中产生了一些新的想法，今后补上读书笔记。</li>
</ul>
</li>
</ol>
<h3 id="不战而胜：独一无二的优质"><a href="#不战而胜：独一无二的优质" class="headerlink" title="不战而胜：独一无二的优质"></a>不战而胜：独一无二的优质</h3><blockquote>
<p>熊谷的“三项求胜方针”：  </p>
<ol>
<li>不能成为第一名的事情，从一开始就不去碰  </li>
<li>不战而胜  </li>
<li>养成求胜的习惯</li>
</ol>
</blockquote>
<p>第一项，不可能成为第一的情况下，勉强展开事业也不太可能获得胜利，就是有把握成为第一的情况下才行动。在他看来只有一项达成了，才能拿下第二项，即在竞争中<strong>不战而胜</strong>。用下面两个场景来解释：</p>
<ul>
<li>在不可能成为第一的领域：<ul>
<li>双方实力相当→价格战（不正当竞争）→恶性循环→员工每天“面对残酷竞争”的心境而感到不幸</li>
</ul>
</li>
<li>在可能成为第一的领域：<ul>
<li>具有压倒性的第一→不参与不正当竞争→以稍高的价格提供优质的服务→员工可以有足够多的收益</li>
</ul>
</li>
</ul>
<p>那么问题来了，怎样才有可能成为领域第一呢？熊谷建议</p>
<blockquote>
<p>我为了要得到这样一个“不战而胜”的理想国，而比谁都要早去开发新事业，也就是“最先到达埋藏着宝藏的无人岛”，就算同业竞争的人增多，我也会朝“与别人相比能够得到压倒性领先”的方向去努力。</p>
</blockquote>
<p>最近一本书《从0到1》很火，书中对“寻找无人岛”提供了更加丰富的方法论。各种性格爱好经历背景排列组合的你一定是与众不同的，你有的别人没有的就是所谓的“无人岛”，所以我觉得自品牌的创建就是挖掘自身非凡的特质，产出独一无二的优质内容，这样才能让受众对你的自品牌产生巨大的兴趣，甚至产生依赖。永远坚持原创，永远不做同质化的东西，宁缺毋滥。没错，很难，但方向是对的，不是吗？唯有多读书，多阅历，慢慢摸索。</p>
<hr>
<h3 id="自媒体和自品牌有什么区别"><a href="#自媒体和自品牌有什么区别" class="headerlink" title="自媒体和自品牌有什么区别"></a>自媒体和自品牌有什么区别</h3><p>自媒体很火现在，本来写了一段自媒体和自品牌的区别，后来又删了，觉得没有必要咬文嚼字，硬要说出区别，那就一点：自媒体与职业生涯可能有关可能无关，也许只是为了社交、好玩；自品牌一定是与职业生涯有关，伴随一生，需要精心维护的。这带来的另一个好处就是，你的职业生涯被记录了，不仅可以约束一个人的言论，也可以帮助你朝着更正能量的方向发展（因为欣赏你的人一定与你持有相同的价值观，越多的人欣赏你，你的价值观就越主流，这并不是说主流价值观就是好的，但大多数情况下，主流价值观给人的感受就是正能量满满；当然，你也可以持有一些非主流，只要做的好，也会成为小众信赖的品牌）。</p>
]]></content>
    </entry>
    
  
  
</search>
